{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import fire\n",
    "import copy\n",
    "import multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.cuda.amp import GradScaler\n",
    "# import deepspeed\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "from pprint import pprint, pformat\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from timm.utils import ModelEmaV3\n",
    "from timm.models import load_checkpoint\n",
    "from timm.utils.model import unwrap_model, get_state_dict\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ModuleNotFoundError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "import sys\n",
    "\n",
    "# sys.path.insert(0, \"..\")\n",
    "sys.path.insert(0, \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-21 02:24:42,234] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/miniconda3/envs/graph_gpt/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/yang/miniconda3/envs/graph_gpt/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlvsym'\n",
      "/home/yang/miniconda3/envs/graph_gpt/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/home/yang/miniconda3/envs/graph_gpt/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/home/yang/miniconda3/envs/graph_gpt/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/home/yang/miniconda3/envs/graph_gpt/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "/home/yang/miniconda3/envs/graph_gpt/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `shm_open'\n",
      "/home/yang/miniconda3/envs/graph_gpt/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `shm_unlink'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "from src.data import (\n",
    "    collator,\n",
    "    vocab_builder,\n",
    "    tokenizer,\n",
    "    read_dataset,\n",
    "    OdpsTableIterableDataset,\n",
    ")\n",
    "from src.models import (\n",
    "    GraphGPTConfig,\n",
    "    GraphGPTCausal,\n",
    "    GraphGPT2Config,\n",
    "    GraphGPT2Causal,\n",
    "    GraphBertConfig,\n",
    "    GraphBertForMaskedLM,\n",
    ")\n",
    "from src.utils import (\n",
    "    conf_utils,\n",
    "    loss_utils,\n",
    "    loader_utils,\n",
    "    tokenizer_utils,\n",
    "    modules_utils,\n",
    "    misc_utils,\n",
    "    print_trainable_parameters,\n",
    "    print_params,\n",
    "    inspect_tokenization_results,\n",
    "    set_up_shuffle_and_sampler,\n",
    "    worker_init_fn_seed,\n",
    ")\n",
    "\n",
    "dict_models = {\n",
    "    \"graphgpt2\": (GraphGPT2Causal, GraphGPT2Config),\n",
    "    \"graphgpt\": (GraphGPTCausal, GraphGPTConfig),\n",
    "    \"graphbert\": (GraphBertForMaskedLM, GraphBertConfig),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir: str = \"../data/OGB\"\n",
    "tables: str = \"\"\n",
    "# deepspeed_config = \"./examples/ds_config2_pt.json\"\n",
    "intermediate_size = 0\n",
    "num_attention_heads = 0\n",
    "hidden_size = 512\n",
    "num_hidden_layers = 8\n",
    "task_type='pretrain'\n",
    "causal_attention = 1\n",
    "lr=3e-4\n",
    "model_type = 'graphgpt'\n",
    "output_dir='./exp/models/pcqm4m-v2/test'\n",
    "pretrain_cpt = '/datalake/datastore1/yang/graph-gpt/exp/models/pcqm4m-v2/medium_ntp/pt_ns_h512_l8_b8192_mpe1024_tk1e9_gelu_pretrain3.3m_nmlm_mrlinear_mtp0.8_0_0.2_lr3e-4_adp0.1_pdp0_edp0_mdp0_lsi0_short_gated_wd0.1'\n",
    "samples_per_saving=1000000\n",
    "\n",
    "batch_size = 1024\n",
    "stack_method = 'short'\n",
    "\n",
    "pack_tokens = 0\n",
    "max_position_embeddings = 1024\n",
    "\n",
    "task_type='pretrain'\n",
    "total_tokens=1e9\n",
    "batch_size = 1024\n",
    "warmup_tokens=1e8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_size: 512 intermediate_size: 2048 num_attention_heads: 8 num_hidden_layers: 8 causal_attention: 1\n",
      "gpu_name: NVIDIA RTX A6000 GraphModel: <class 'src.models.graphgpt.modeling_graphgpt.GraphGPTCausal'> GraphModelConfig: <class 'src.models.graphgpt.configuration_graphgpt.GraphGPTConfig'>\n"
     ]
    }
   ],
   "source": [
    "use_tb_writer = False           # use tensorboard writer\n",
    "use_ema = False # False # use exponential moving average to smooth model\n",
    "use_deepspeed = False # True # use deepspeed for training, good to set scheduler\n",
    "if (intermediate_size == 0) and (num_attention_heads == 0): # True\n",
    "    (\n",
    "        hidden_size,\n",
    "        intermediate_size,\n",
    "        num_attention_heads,\n",
    "        num_hidden_layers,\n",
    "    ) = modules_utils.set_up_model_architect(\n",
    "        hidden_size=hidden_size, num_hidden_layers=num_hidden_layers # 768 24 related to model names intermediate_size = hidden_size * 4, num_attention_heads = hidden_size // 64\n",
    "    )# 768 3072 12 24\n",
    "causal_attention = 0 if task_type == \"pretrain-mlm\" else causal_attention\n",
    "print('hidden_size:', hidden_size, 'intermediate_size:', intermediate_size, 'num_attention_heads:', num_attention_heads, 'num_hidden_layers:', num_hidden_layers, 'causal_attention:', causal_attention) # 768 3072 12 24 1\n",
    "\n",
    "\n",
    "# #########################\n",
    "# betas = (0.9, 0.95) # used in AdamW optimizer, important for config beta\n",
    "# #########################\n",
    "# # lr * 0.1 -> from llama2 pre-train settings\n",
    "# min_lr = lr * 0.1 if use_deepspeed else 0    # used in scheduler, when not using deepspeed.\n",
    "# #########################\n",
    "gpu_name = torch.cuda.get_device_name()\n",
    "GraphModel, GraphModelConfig = dict_models[model_type] # Not instantiate yet\n",
    "print('gpu_name:', gpu_name, 'GraphModel:', GraphModel, 'GraphModelConfig:', GraphModelConfig) \n",
    "\n",
    "if os.path.exists(os.path.join(output_dir, \"log.csv\")):\n",
    "    print(\n",
    "        f\"log file {os.path.join(output_dir, 'log.csv')} exists, resume training from {output_dir} instead of initializing from pre-train ckp {pretrain_cpt}!\"\n",
    "    )\n",
    "    pretrain_cpt = output_dir\n",
    "\n",
    "\n",
    "# # 0. init distributed train and get gpu/device info\n",
    "# dist.init_process_group(backend=\"nccl\", init_method=\"env://\")  # for distributed training\n",
    "# dist.barrier() # for sync training\n",
    "# world_size = dist.get_world_size() # 1 # number of GPUs\n",
    "# rank = dist.get_rank() # 0 # current GPU index\n",
    "# local_rank = os.environ.get(\"LOCAL_RANK\") # 0 # current GPU index local to the node\n",
    "# print(f\"\\nworld size: {world_size}, rank: {rank}, local rank: {local_rank}\") # 1 0 0\n",
    "# rnd_seed = torch.random.initial_seed() - rank\n",
    "# random.seed(rnd_seed)\n",
    "# print(f\"seed random with {rnd_seed}\") # 1234\n",
    "# steps_per_saving = samples_per_saving // (world_size * batch_size) # 1000000 // (1 * 1024) = 976\n",
    "# print(f\"\\nsteps_per_saving: {steps_per_saving}\") # 976\n",
    "# params = print_params(**locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer config loading\n",
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "file_path = \"./zhang_test/tokenizer_config.json\"\n",
    "with open(file_path, \"r\") as json_file:\n",
    "    tokenizer_config = json.load(json_file)\n",
    "\n",
    "# Print the loaded data\n",
    "# pprint(tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacked_feat: 13, next_n_token: 13, embed_dim: 0\n"
     ]
    }
   ],
   "source": [
    "# 1.1 read configuration\n",
    "assert \"pretrain\" in tokenizer_config[\"task_type\"]\n",
    "assert (\n",
    "    tokenizer_config[\"semantics\"][\"attr_assignment\"]   # first\n",
    "    in tokenizer_utils.ATTR_ASSIGNMENT_TYPES   # ATTR_ASSIGNMENT_TYPES = {\"first\", \"last\", \"random\", \"all\", \"mix\"}\n",
    ")\n",
    "# pprint(tokenizer_config)\n",
    "if tokenizer_config[\"tokenizer_class\"] == \"StackedGSTTokenizer\":\n",
    "    attr_dim = (\n",
    "        tokenizer_config[\"semantics\"][\"edge\"][\"dim\"] # 3\n",
    "        + tokenizer_config[\"semantics\"][\"node\"][\"dim\"] # 9\n",
    "    ) # 12\n",
    "    assert stack_method in (\"short\", \"long\", None), f\"stack_method: {stack_method}\" # short\n",
    "    if tokenizer_config[\"structure\"][\"edge\"][\"remove_edge_type_token\"]: # True\n",
    "        stacked_feat = 1 + attr_dim\n",
    "    else:\n",
    "        stacked_feat = 2 + attr_dim\n",
    "    next_n_token = stacked_feat\n",
    "else:\n",
    "    stacked_feat = 1\n",
    "    next_n_token = 1 # maybe how many pack of tokens to predict\n",
    "embed_dim = tokenizer_config[\"semantics\"][\"node\"].get(\n",
    "    \"embed_dim\", 0\n",
    ") + tokenizer_config[\"semantics\"][\"edge\"].get(\"embed_dim\", 0) # 0\n",
    "print(\n",
    "    f\"stacked_feat: {stacked_feat}, next_n_token: {next_n_token}, embed_dim: {embed_dim}\" # 13 13 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-21 02:24:44.116160] Loading vocab from ./data/OGB/pcqm4m-v2/vocab512_stacked ...\n",
      "[2024-12-21 02:24:44.118654]\n",
      "{   '0': 22,\n",
      "    '1': 23,\n",
      "    '10': 32,\n",
      "    '100': 122,\n",
      "    '101': 123,\n",
      "    '102': 124,\n",
      "    '103': 125,\n",
      "    '104': 126,\n",
      "    '105': 127,\n",
      "    '106': 128,\n",
      "    '107': 129,\n",
      "    '108': 130,\n",
      "    '109': 131,\n",
      "    '11': 33,\n",
      "    '110': 132,\n",
      "    '111': 133,\n",
      "    '112': 134,\n",
      "    '113': 135,\n",
      "    '114': 136,\n",
      "    '115': 137,\n",
      "    '116': 138,\n",
      "    '117': 139,\n",
      "    '118': 140,\n",
      "    '119': 141,\n",
      "    '12': 34,\n",
      "    '120': 142,\n",
      "    '121': 143,\n",
      "    '122': 144,\n",
      "    '123': 145,\n",
      "    '124': 146,\n",
      "    '125': 147,\n",
      "    '126': 148,\n",
      "    '127': 149,\n",
      "    '128': 150,\n",
      "    '129': 151,\n",
      "    '13': 35,\n",
      "    '130': 152,\n",
      "    '131': 153,\n",
      "    '132': 154,\n",
      "    '133': 155,\n",
      "    '134': 156,\n",
      "    '135': 157,\n",
      "    '136': 158,\n",
      "    '137': 159,\n",
      "    '138': 160,\n",
      "    '139': 161,\n",
      "    '14': 36,\n",
      "    '140': 162,\n",
      "    '141': 163,\n",
      "    '142': 164,\n",
      "    '143': 165,\n",
      "    '144': 166,\n",
      "    '145': 167,\n",
      "    '146': 168,\n",
      "    '147': 169,\n",
      "    '148': 170,\n",
      "    '149': 171,\n",
      "    '15': 37,\n",
      "    '150': 172,\n",
      "    '151': 173,\n",
      "    '152': 174,\n",
      "    '153': 175,\n",
      "    '154': 176,\n",
      "    '155': 177,\n",
      "    '156': 178,\n",
      "    '157': 179,\n",
      "    '158': 180,\n",
      "    '159': 181,\n",
      "    '16': 38,\n",
      "    '160': 182,\n",
      "    '161': 183,\n",
      "    '162': 184,\n",
      "    '163': 185,\n",
      "    '164': 186,\n",
      "    '165': 187,\n",
      "    '166': 188,\n",
      "    '167': 189,\n",
      "    '168': 190,\n",
      "    '169': 191,\n",
      "    '17': 39,\n",
      "    '170': 192,\n",
      "    '171': 193,\n",
      "    '172': 194,\n",
      "    '173': 195,\n",
      "    '174': 196,\n",
      "    '175': 197,\n",
      "    '176': 198,\n",
      "    '177': 199,\n",
      "    '178': 200,\n",
      "    '179': 201,\n",
      "    '18': 40,\n",
      "    '180': 202,\n",
      "    '181': 203,\n",
      "    '182': 204,\n",
      "    '183': 205,\n",
      "    '184': 206,\n",
      "    '185': 207,\n",
      "    '186': 208,\n",
      "    '187': 209,\n",
      "    '188': 210,\n",
      "    '189': 211,\n",
      "    '19': 41,\n",
      "    '190': 212,\n",
      "    '191': 213,\n",
      "    '192': 214,\n",
      "    '193': 215,\n",
      "    '194': 216,\n",
      "    '195': 217,\n",
      "    '196': 218,\n",
      "    '197': 219,\n",
      "    '198': 220,\n",
      "    '199': 221,\n",
      "    '2': 24,\n",
      "    '20': 42,\n",
      "    '200': 222,\n",
      "    '201': 223,\n",
      "    '202': 224,\n",
      "    '203': 225,\n",
      "    '204': 226,\n",
      "    '205': 227,\n",
      "    '206': 228,\n",
      "    '207': 229,\n",
      "    '208': 230,\n",
      "    '209': 231,\n",
      "    '21': 43,\n",
      "    '210': 232,\n",
      "    '211': 233,\n",
      "    '212': 234,\n",
      "    '213': 235,\n",
      "    '214': 236,\n",
      "    '215': 237,\n",
      "    '216': 238,\n",
      "    '217': 239,\n",
      "    '218': 240,\n",
      "    '219': 241,\n",
      "    '22': 44,\n",
      "    '220': 242,\n",
      "    '221': 243,\n",
      "    '222': 244,\n",
      "    '223': 245,\n",
      "    '224': 246,\n",
      "    '225': 247,\n",
      "    '226': 248,\n",
      "    '227': 249,\n",
      "    '228': 250,\n",
      "    '229': 251,\n",
      "    '23': 45,\n",
      "    '230': 252,\n",
      "    '231': 253,\n",
      "    '232': 254,\n",
      "    '233': 255,\n",
      "    '234': 256,\n",
      "    '235': 257,\n",
      "    '236': 258,\n",
      "    '237': 259,\n",
      "    '238': 260,\n",
      "    '239': 261,\n",
      "    '24': 46,\n",
      "    '240': 262,\n",
      "    '241': 263,\n",
      "    '242': 264,\n",
      "    '243': 265,\n",
      "    '244': 266,\n",
      "    '245': 267,\n",
      "    '246': 268,\n",
      "    '247': 269,\n",
      "    '248': 270,\n",
      "    '249': 271,\n",
      "    '25': 47,\n",
      "    '250': 272,\n",
      "    '251': 273,\n",
      "    '252': 274,\n",
      "    '253': 275,\n",
      "    '254': 276,\n",
      "    '255': 277,\n",
      "    '256': 278,\n",
      "    '257': 279,\n",
      "    '258': 280,\n",
      "    '259': 281,\n",
      "    '26': 48,\n",
      "    '260': 282,\n",
      "    '261': 283,\n",
      "    '262': 284,\n",
      "    '263': 285,\n",
      "    '264': 286,\n",
      "    '265': 287,\n",
      "    '266': 288,\n",
      "    '267': 289,\n",
      "    '268': 290,\n",
      "    '269': 291,\n",
      "    '27': 49,\n",
      "    '270': 292,\n",
      "    '271': 293,\n",
      "    '272': 294,\n",
      "    '273': 295,\n",
      "    '274': 296,\n",
      "    '275': 297,\n",
      "    '276': 298,\n",
      "    '277': 299,\n",
      "    '278': 300,\n",
      "    '279': 301,\n",
      "    '28': 50,\n",
      "    '280': 302,\n",
      "    '281': 303,\n",
      "    '282': 304,\n",
      "    '283': 305,\n",
      "    '284': 306,\n",
      "    '285': 307,\n",
      "    '286': 308,\n",
      "    '287': 309,\n",
      "    '288': 310,\n",
      "    '289': 311,\n",
      "    '29': 51,\n",
      "    '290': 312,\n",
      "    '291': 313,\n",
      "    '292': 314,\n",
      "    '293': 315,\n",
      "    '294': 316,\n",
      "    '295': 317,\n",
      "    '296': 318,\n",
      "    '297': 319,\n",
      "    '298': 320,\n",
      "    '299': 321,\n",
      "    '3': 25,\n",
      "    '30': 52,\n",
      "    '300': 322,\n",
      "    '301': 323,\n",
      "    '302': 324,\n",
      "    '303': 325,\n",
      "    '304': 326,\n",
      "    '305': 327,\n",
      "    '306': 328,\n",
      "    '307': 329,\n",
      "    '308': 330,\n",
      "    '309': 331,\n",
      "    '31': 53,\n",
      "    '310': 332,\n",
      "    '311': 333,\n",
      "    '312': 334,\n",
      "    '313': 335,\n",
      "    '314': 336,\n",
      "    '315': 337,\n",
      "    '316': 338,\n",
      "    '317': 339,\n",
      "    '318': 340,\n",
      "    '319': 341,\n",
      "    '32': 54,\n",
      "    '320': 342,\n",
      "    '321': 343,\n",
      "    '322': 344,\n",
      "    '323': 345,\n",
      "    '324': 346,\n",
      "    '325': 347,\n",
      "    '326': 348,\n",
      "    '327': 349,\n",
      "    '328': 350,\n",
      "    '329': 351,\n",
      "    '33': 55,\n",
      "    '330': 352,\n",
      "    '331': 353,\n",
      "    '332': 354,\n",
      "    '333': 355,\n",
      "    '334': 356,\n",
      "    '335': 357,\n",
      "    '336': 358,\n",
      "    '337': 359,\n",
      "    '338': 360,\n",
      "    '339': 361,\n",
      "    '34': 56,\n",
      "    '340': 362,\n",
      "    '341': 363,\n",
      "    '342': 364,\n",
      "    '343': 365,\n",
      "    '344': 366,\n",
      "    '345': 367,\n",
      "    '346': 368,\n",
      "    '347': 369,\n",
      "    '348': 370,\n",
      "    '349': 371,\n",
      "    '35': 57,\n",
      "    '350': 372,\n",
      "    '351': 373,\n",
      "    '352': 374,\n",
      "    '353': 375,\n",
      "    '354': 376,\n",
      "    '355': 377,\n",
      "    '356': 378,\n",
      "    '357': 379,\n",
      "    '358': 380,\n",
      "    '359': 381,\n",
      "    '36': 58,\n",
      "    '360': 382,\n",
      "    '361': 383,\n",
      "    '362': 384,\n",
      "    '363': 385,\n",
      "    '364': 386,\n",
      "    '365': 387,\n",
      "    '366': 388,\n",
      "    '367': 389,\n",
      "    '368': 390,\n",
      "    '369': 391,\n",
      "    '37': 59,\n",
      "    '370': 392,\n",
      "    '371': 393,\n",
      "    '372': 394,\n",
      "    '373': 395,\n",
      "    '374': 396,\n",
      "    '375': 397,\n",
      "    '376': 398,\n",
      "    '377': 399,\n",
      "    '378': 400,\n",
      "    '379': 401,\n",
      "    '38': 60,\n",
      "    '380': 402,\n",
      "    '381': 403,\n",
      "    '382': 404,\n",
      "    '383': 405,\n",
      "    '384': 406,\n",
      "    '385': 407,\n",
      "    '386': 408,\n",
      "    '387': 409,\n",
      "    '388': 410,\n",
      "    '389': 411,\n",
      "    '39': 61,\n",
      "    '390': 412,\n",
      "    '391': 413,\n",
      "    '392': 414,\n",
      "    '393': 415,\n",
      "    '394': 416,\n",
      "    '395': 417,\n",
      "    '396': 418,\n",
      "    '397': 419,\n",
      "    '398': 420,\n",
      "    '399': 421,\n",
      "    '4': 26,\n",
      "    '40': 62,\n",
      "    '400': 422,\n",
      "    '401': 423,\n",
      "    '402': 424,\n",
      "    '403': 425,\n",
      "    '404': 426,\n",
      "    '405': 427,\n",
      "    '406': 428,\n",
      "    '407': 429,\n",
      "    '408': 430,\n",
      "    '409': 431,\n",
      "    '41': 63,\n",
      "    '410': 432,\n",
      "    '411': 433,\n",
      "    '412': 434,\n",
      "    '413': 435,\n",
      "    '414': 436,\n",
      "    '415': 437,\n",
      "    '416': 438,\n",
      "    '417': 439,\n",
      "    '418': 440,\n",
      "    '419': 441,\n",
      "    '42': 64,\n",
      "    '420': 442,\n",
      "    '421': 443,\n",
      "    '422': 444,\n",
      "    '423': 445,\n",
      "    '424': 446,\n",
      "    '425': 447,\n",
      "    '426': 448,\n",
      "    '427': 449,\n",
      "    '428': 450,\n",
      "    '429': 451,\n",
      "    '43': 65,\n",
      "    '430': 452,\n",
      "    '431': 453,\n",
      "    '432': 454,\n",
      "    '433': 455,\n",
      "    '434': 456,\n",
      "    '435': 457,\n",
      "    '436': 458,\n",
      "    '437': 459,\n",
      "    '438': 460,\n",
      "    '439': 461,\n",
      "    '44': 66,\n",
      "    '440': 462,\n",
      "    '441': 463,\n",
      "    '442': 464,\n",
      "    '443': 465,\n",
      "    '444': 466,\n",
      "    '445': 467,\n",
      "    '446': 468,\n",
      "    '447': 469,\n",
      "    '448': 470,\n",
      "    '449': 471,\n",
      "    '45': 67,\n",
      "    '450': 472,\n",
      "    '451': 473,\n",
      "    '452': 474,\n",
      "    '453': 475,\n",
      "    '454': 476,\n",
      "    '455': 477,\n",
      "    '456': 478,\n",
      "    '457': 479,\n",
      "    '458': 480,\n",
      "    '459': 481,\n",
      "    '46': 68,\n",
      "    '460': 482,\n",
      "    '461': 483,\n",
      "    '462': 484,\n",
      "    '463': 485,\n",
      "    '464': 486,\n",
      "    '465': 487,\n",
      "    '466': 488,\n",
      "    '467': 489,\n",
      "    '468': 490,\n",
      "    '469': 491,\n",
      "    '47': 69,\n",
      "    '470': 492,\n",
      "    '471': 493,\n",
      "    '472': 494,\n",
      "    '473': 495,\n",
      "    '474': 496,\n",
      "    '475': 497,\n",
      "    '476': 498,\n",
      "    '477': 499,\n",
      "    '478': 500,\n",
      "    '479': 501,\n",
      "    '48': 70,\n",
      "    '480': 502,\n",
      "    '481': 503,\n",
      "    '482': 504,\n",
      "    '483': 505,\n",
      "    '484': 506,\n",
      "    '485': 507,\n",
      "    '486': 508,\n",
      "    '487': 509,\n",
      "    '488': 510,\n",
      "    '489': 511,\n",
      "    '49': 71,\n",
      "    '490': 512,\n",
      "    '491': 513,\n",
      "    '492': 514,\n",
      "    '493': 515,\n",
      "    '494': 516,\n",
      "    '495': 517,\n",
      "    '496': 518,\n",
      "    '497': 519,\n",
      "    '498': 520,\n",
      "    '499': 521,\n",
      "    '5': 27,\n",
      "    '50': 72,\n",
      "    '500': 522,\n",
      "    '501': 523,\n",
      "    '502': 524,\n",
      "    '503': 525,\n",
      "    '504': 526,\n",
      "    '505': 527,\n",
      "    '506': 528,\n",
      "    '507': 529,\n",
      "    '508': 530,\n",
      "    '509': 531,\n",
      "    '51': 73,\n",
      "    '510': 532,\n",
      "    '511': 533,\n",
      "    '52': 74,\n",
      "    '53': 75,\n",
      "    '54': 76,\n",
      "    '55': 77,\n",
      "    '56': 78,\n",
      "    '57': 79,\n",
      "    '58': 80,\n",
      "    '59': 81,\n",
      "    '6': 28,\n",
      "    '60': 82,\n",
      "    '61': 83,\n",
      "    '62': 84,\n",
      "    '63': 85,\n",
      "    '64': 86,\n",
      "    '65': 87,\n",
      "    '66': 88,\n",
      "    '67': 89,\n",
      "    '68': 90,\n",
      "    '69': 91,\n",
      "    '7': 29,\n",
      "    '70': 92,\n",
      "    '71': 93,\n",
      "    '72': 94,\n",
      "    '73': 95,\n",
      "    '74': 96,\n",
      "    '75': 97,\n",
      "    '76': 98,\n",
      "    '77': 99,\n",
      "    '78': 100,\n",
      "    '79': 101,\n",
      "    '8': 30,\n",
      "    '80': 102,\n",
      "    '81': 103,\n",
      "    '82': 104,\n",
      "    '83': 105,\n",
      "    '84': 106,\n",
      "    '85': 107,\n",
      "    '86': 108,\n",
      "    '87': 109,\n",
      "    '88': 110,\n",
      "    '89': 111,\n",
      "    '9': 31,\n",
      "    '90': 112,\n",
      "    '91': 113,\n",
      "    '92': 114,\n",
      "    '93': 115,\n",
      "    '94': 116,\n",
      "    '95': 117,\n",
      "    '96': 118,\n",
      "    '97': 119,\n",
      "    '98': 120,\n",
      "    '99': 121,\n",
      "    '<->': 546,\n",
      "    '<.>': 545,\n",
      "    '<0>': 547,\n",
      "    '<1>': 548,\n",
      "    '<2>': 549,\n",
      "    '<3>': 550,\n",
      "    '<4>': 551,\n",
      "    '<5>': 552,\n",
      "    '<6>': 553,\n",
      "    '<7>': 554,\n",
      "    '<8>': 555,\n",
      "    '<9>': 556,\n",
      "    '<bos>': 20,\n",
      "    '<e>': 544,\n",
      "    '<edge_bi>': 17,\n",
      "    '<edge_in>': 15,\n",
      "    '<edge_jump>': 18,\n",
      "    '<edge_out>': 16,\n",
      "    '<eos>': 19,\n",
      "    '<gsum>': 14,\n",
      "    '<icl>': 2,\n",
      "    '<label_pad>': -100,\n",
      "    '<mask>': 1,\n",
      "    '<new>': 21,\n",
      "    '<sep>': 3,\n",
      "    'molecule#edge#0': 740,\n",
      "    'molecule#edge#0#0': 743,\n",
      "    'molecule#edge#0#1': 744,\n",
      "    'molecule#edge#0#2': 745,\n",
      "    'molecule#edge#0#3': 746,\n",
      "    'molecule#edge#0#4': 747,\n",
      "    'molecule#edge#1': 741,\n",
      "    'molecule#edge#1#0': 748,\n",
      "    'molecule#edge#1#1': 749,\n",
      "    'molecule#edge#1#2': 750,\n",
      "    'molecule#edge#1#3': 751,\n",
      "    'molecule#edge#1#4': 752,\n",
      "    'molecule#edge#1#5': 753,\n",
      "    'molecule#edge#2': 742,\n",
      "    'molecule#edge#2#0': 754,\n",
      "    'molecule#edge#2#1': 755,\n",
      "    'molecule#node#0': 557,\n",
      "    'molecule#node#0#0': 566,\n",
      "    'molecule#node#0#1': 567,\n",
      "    'molecule#node#0#10': 568,\n",
      "    'molecule#node#0#100': 569,\n",
      "    'molecule#node#0#101': 570,\n",
      "    'molecule#node#0#102': 571,\n",
      "    'molecule#node#0#103': 572,\n",
      "    'molecule#node#0#104': 573,\n",
      "    'molecule#node#0#105': 574,\n",
      "    'molecule#node#0#106': 575,\n",
      "    'molecule#node#0#107': 576,\n",
      "    'molecule#node#0#108': 577,\n",
      "    'molecule#node#0#109': 578,\n",
      "    'molecule#node#0#11': 579,\n",
      "    'molecule#node#0#110': 580,\n",
      "    'molecule#node#0#111': 581,\n",
      "    'molecule#node#0#112': 582,\n",
      "    'molecule#node#0#113': 583,\n",
      "    'molecule#node#0#114': 584,\n",
      "    'molecule#node#0#115': 585,\n",
      "    'molecule#node#0#116': 586,\n",
      "    'molecule#node#0#117': 587,\n",
      "    'molecule#node#0#118': 588,\n",
      "    'molecule#node#0#12': 589,\n",
      "    'molecule#node#0#13': 590,\n",
      "    'molecule#node#0#14': 591,\n",
      "    'molecule#node#0#15': 592,\n",
      "    'molecule#node#0#16': 593,\n",
      "    'molecule#node#0#17': 594,\n",
      "    'molecule#node#0#18': 595,\n",
      "    'molecule#node#0#19': 596,\n",
      "    'molecule#node#0#2': 597,\n",
      "    'molecule#node#0#20': 598,\n",
      "    'molecule#nod ......\n",
      "label token id to be converted to -100 is {2}\n"
     ]
    }
   ],
   "source": [
    "add_eos = False\n",
    "rank = 0\n",
    "stack_method = \"short\"\n",
    "# 1.3 build vocab and then init tokenizer from the tokenization config\n",
    "tokenizer_cls = getattr(tokenizer, tokenizer_config[\"tokenizer_class\"]) # StackGSTTokenizer, custom defined\n",
    "gtokenizer = tokenizer_cls(\n",
    "    tokenizer_config, add_eos=add_eos, stack_method=stack_method # instantiate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<src.data.tokenizer.StackedGSTTokenizer object at 0x7f52a0445760>\n"
     ]
    }
   ],
   "source": [
    "print(gtokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./zhang_test/model_config.pkl\", \"rb\") as file:  # \"rb\" mode for reading binary\n",
    "    config = pickle.load(file)\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_deepspeed = True\n",
    "\n",
    "# # 2.2 create model\n",
    "# if use_deepspeed:\n",
    "#     deepspeed.init_distributed(\n",
    "#         dist_backend=\"nccl\", rank=rank, world_size=world_size\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT Applying dropout in backbone transformer\n",
      "Next-token-prediction changed to next/masked-13-tokens-prediction!\n",
      "trainable params: 37751808 || all params: 37751808 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "model = GraphModel(config)\n",
    "\n",
    "\n",
    "# model.gradient_checkpointing_enable()\n",
    "# silence the warnings. Please re-enable for inference!\n",
    "model.config.use_cache = False\n",
    "print_trainable_parameters(model) # 235368960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained weights from ckp /datalake/datastore1/yang/graph-gpt/exp/models/pcqm4m-v2/medium_ntp/pt_ns_h512_l8_b8192_mpe1024_tk1e9_gelu_pretrain3.3m_nmlm_mrlinear_mtp0.8_0_0.2_lr3e-4_adp0.1_pdp0_edp0_mdp0_lsi0_short_gated_wd0.1/epoch_51\n",
      "inar:  [Errno 2] No such file or directory: '/datalake/datastore1/yang/graph-gpt/exp/models/pcqm4m-v2/medium_ntp/pt_ns_h512_l8_b8192_mpe1024_tk1e9_gelu_pretrain3.3m_nmlm_mrlinear_mtp0.8_0_0.2_lr3e-4_adp0.1_pdp0_edp0_mdp0_lsi0_short_gated_wd0.1/epoch_51/model.pt'\n",
      "Processing zero checkpoint '/datalake/datastore1/yang/graph-gpt/exp/models/pcqm4m-v2/medium_ntp/pt_ns_h512_l8_b8192_mpe1024_tk1e9_gelu_pretrain3.3m_nmlm_mrlinear_mtp0.8_0_0.2_lr3e-4_adp0.1_pdp0_edp0_mdp0_lsi0_short_gated_wd0.1/epoch_51/global_step48830'\n",
      "Detected checkpoint of type zero stage 2, world_size: 1\n",
      "Parsing checkpoint created by deepspeed==0.15.1\n",
      "Reconstructed fp32 state dict with 77 params 37751808 elements\n",
      "[2024-12-21 02:24:45.703601] load ckp using DeepSpeed API `get_fp32_state_dict_from_zero_checkpoint`\n",
      "[2024-12-21 02:24:45.736843] init model params using pytorch `load_state_dict`\n",
      "missing keys: []\n",
      "unexpected_keys: []\n",
      "After loading weights from ckp:\n",
      "GraphGPTConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 20,\n",
      "  \"causal_attention\": true,\n",
      "  \"cls_token_id\": null,\n",
      "  \"dropout\": 0,\n",
      "  \"embed_dim\": 0,\n",
      "  \"embed_pdrop\": 0,\n",
      "  \"eos_token_id\": 19,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_scale_init_value\": 0,\n",
      "  \"loss_type\": null,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"mlp\": [],\n",
      "  \"mlp_pdrop\": 0,\n",
      "  \"model_type\": \"graphgpt\",\n",
      "  \"next_n_token\": 13,\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 8,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_neg\": null,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"path_pdrop\": 0,\n",
      "  \"pooling_method\": \"last\",\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"stack_method\": \"short\",\n",
      "  \"stacked_feat\": 13,\n",
      "  \"stacked_feat_agg_method\": \"gated\",\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 756\n",
      "}\n",
      "\n",
      "model-type: torch.float32\n",
      "\n",
      "GraphGPTCausal(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(756, 512, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (up_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (down_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (act_fn): GELUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=756, bias=False)\n",
      "  (stacked_feat_agg): StackedFeatAggregation(stacked_feat=13, hidden_size=512)\n",
      "  (next_n_token_head): Linear(in_features=512, out_features=6656, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2.21 load from ckp IF provided existing ckp and NOT resume from the ckp\n",
    "ckp, _ = misc_utils.get_latest_ckp(pretrain_cpt)\n",
    "print(f\"Loading pretrained weights from ckp {ckp}\")\n",
    "try:\n",
    "    # fn_model = os.path.join(ckp, \"../model_ema_best.pt\")\n",
    "    # if not os.path.isfile(fn_model):\n",
    "    fn_model = os.path.join(ckp, \"model.pt\")\n",
    "    stat_dict = torch.load(fn_model)\n",
    "    stat_dict = {\n",
    "        (k[7:] if k.startswith(\"module.\") else k): v for k, v in stat_dict.items()\n",
    "    }\n",
    "    print(f\"[{datetime.now()}] load ckp using torch API from:\\n{fn_model}\")\n",
    "except Exception as inst:\n",
    "    # print(type(inst))\n",
    "    # print(inst.args)\n",
    "    print(\"inar: \", inst)\n",
    "    from deepspeed.utils.zero_to_fp32 import (\n",
    "        get_fp32_state_dict_from_zero_checkpoint,\n",
    "    )\n",
    "    stat_dict = get_fp32_state_dict_from_zero_checkpoint(ckp)\n",
    "    print(\n",
    "        f\"[{datetime.now()}] load ckp using DeepSpeed API `get_fp32_state_dict_from_zero_checkpoint`\"\n",
    "    )\n",
    "\n",
    "for key in list(stat_dict.keys()):\n",
    "    if (\"score\" in key) and skip_keys:\n",
    "        stat_dict.pop(key)\n",
    "        print(f\"pop key {key} in stat_dict!\")\n",
    "missing_keys, unexpected_keys = model.load_state_dict(stat_dict, strict=True)\n",
    "print(\n",
    "    f\"[{datetime.now()}] init model params using pytorch `load_state_dict`\\n\"\n",
    "    f\"missing keys: {missing_keys}\\n\"\n",
    "    f\"unexpected_keys: {unexpected_keys}\\n\"\n",
    "    f\"After loading weights from ckp:\\n{model.config}\\nmodel-type: {model.dtype}\\n\\n{model}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"device: {device}\")\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset PCQM4Mv2 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dataset._data -> Data(edge_index=[2, 109093626], edge_attr=[109093626, 3], x=[52970652, 9], y=[3746620])\n",
      "\n",
      "Raw indices: 3378606, Removed indices: 0, New indices: 3378606\n",
      "\n",
      "Raw indices: 73545, Removed indices: 0, New indices: 73545\n",
      "Using all valid data as valid: 73545, and last half of valid data as test: 36773!\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "\n",
      "[2024-12-21 02:24:53.845565] NOT RESET samples of GraphsMapDataset of 3378606 graphs for epoch None!\n",
      "idx_tuple: None\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "\n",
      "[2024-12-21 02:24:53.882407] NOT RESET samples of GraphsMapDataset of 73545 graphs for epoch None!\n",
      "idx_tuple: None\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "\n",
      "[2024-12-21 02:24:53.899264] NOT RESET samples of GraphsMapDataset of 36773 graphs for epoch None!\n",
      "idx_tuple: None\n",
      "Split dataset based on given train/valid/test index!\n",
      "Train: 3378606, Valid: 73545, Test: 36773!\n",
      "(3561983, Data(edge_index=[2, 34], edge_attr=[34, 3], x=[16, 9], y=[1], num_nodes=16, idx=3561983, idx_of_ds=0))\n"
     ]
    }
   ],
   "source": [
    "# 1.2 get graph dataset\n",
    "train_dataset, valid_dataset, test_dataset, raw_dataset = read_dataset(\n",
    "    name=tokenizer_config[\"dataset\"],   # PCQM4Mv2\n",
    "    # for local data file reading\n",
    "    data_dir=data_dir,   # './data/OGB'\n",
    "    sampling_config=tokenizer_config[\"sampling\"],    # None\n",
    "    # for odps data reading\n",
    "    table=tables,   # \"\"\n",
    "    edge_dim=tokenizer_config[\"semantics\"][\"edge\"][\"dim\"],    # 3\n",
    "    node_dim=tokenizer_config[\"semantics\"][\"node\"][\"dim\"],    # 9\n",
    "    mode=\"train\",\n",
    "    # general\n",
    "    # pretrain_mode=True,\n",
    "    return_valid_test=True,\n",
    "    ensemble_datasets=tokenizer_config.get(\"ensemble_datasets\", []),    # []\n",
    ")\n",
    "reset_samples_per_epoch = (   # what is this  # None for PCQM4Mv2\n",
    "    test_dataset.reset_samples_per_epoch\n",
    "    if hasattr(test_dataset, \"reset_samples_per_epoch\")\n",
    "    else False\n",
    ")\n",
    "if isinstance(test_dataset, IterableDataset):\n",
    "    print(next(iter(test_dataset))) \n",
    "else: # True\n",
    "    idx = test_dataset.sampler[0] # (0, Data(edge_index=[2, 40], edge_attr=[40, 3], x=[18, 9], y=[1, 1], num_nodes=18, idx=0, idx_of_ds=0))\n",
    "    print(test_dataset[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<src.data.dataset_map.GraphsMapDataset object at 0x7f5174cf3640>\n",
      "length of test dataset: 36773 length of train dataset: 3378606 length of valid dataset: 73545\n",
      "(1, Data(edge_index=[2, 34], edge_attr=[34, 3], x=[17, 9], y=[1], num_nodes=17, idx=1, idx_of_ds=0))\n",
      "(2, Data(edge_index=[2, 32], edge_attr=[32, 3], x=[16, 9], y=[1], num_nodes=16, idx=2, idx_of_ds=0))\n",
      "####################################################################################################\n",
      "example\n",
      "edge_index:  tensor([[ 6,  2,  2, 16, 16, 13, 13, 10, 10,  0,  0,  5, 10,  8,  8, 11, 11,  3,\n",
      "          3, 12, 12,  7,  7,  9,  9,  1,  1,  4,  1, 15,  9, 14,  3, 16],\n",
      "        [ 2,  6, 16,  2, 13, 16, 10, 13,  0, 10,  5,  0,  8, 10, 11,  8,  3, 11,\n",
      "         12,  3,  7, 12,  9,  7,  1,  9,  4,  1, 15,  1, 14,  9, 16,  3]])\n",
      "edge_attr:  tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [1, 2, 1],\n",
      "        [1, 2, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [1, 0, 1],\n",
      "        [1, 0, 1],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1]])\n",
      "x:  tensor([[5, 0, 4, 5, 3, 0, 2, 0, 0],\n",
      "        [5, 0, 3, 5, 0, 0, 1, 0, 0],\n",
      "        [5, 0, 3, 5, 1, 0, 1, 1, 1],\n",
      "        [5, 0, 4, 5, 3, 0, 2, 0, 0],\n",
      "        [5, 0, 4, 5, 3, 0, 2, 0, 0],\n",
      "        [5, 0, 3, 5, 0, 0, 1, 1, 1],\n",
      "        [7, 0, 2, 5, 0, 0, 1, 0, 0],\n",
      "        [5, 0, 3, 5, 0, 0, 1, 1, 1],\n",
      "        [6, 0, 3, 5, 0, 0, 1, 0, 0],\n",
      "        [7, 0, 2, 5, 0, 0, 1, 0, 0],\n",
      "        [5, 0, 3, 5, 1, 0, 1, 1, 1],\n",
      "        [5, 0, 3, 5, 1, 0, 1, 1, 1],\n",
      "        [5, 0, 4, 5, 3, 0, 2, 0, 0],\n",
      "        [7, 0, 1, 5, 0, 0, 1, 0, 0],\n",
      "        [5, 0, 3, 5, 0, 0, 1, 1, 1],\n",
      "        [5, 0, 3, 5, 1, 0, 1, 0, 0],\n",
      "        [5, 0, 3, 5, 1, 0, 1, 0, 0]])\n",
      "y:  tensor([4.4110])\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)\n",
    "print('length of test dataset:', len(test_dataset), 'length of train dataset:', len(train_dataset), 'length of valid dataset:', len(valid_dataset))\n",
    "print(test_dataset[1])\n",
    "print(test_dataset[2])\n",
    "print(\"#\" * 100)\n",
    "print(\"example\")\n",
    "print(\"edge_index: \", test_dataset[1][1].edge_index)\n",
    "print(\"edge_attr: \", test_dataset[1][1].edge_attr)\n",
    "print(\"x: \", test_dataset[1][1].x)\n",
    "print(\"y: \", test_dataset[1][1].y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=CCN(C=Cc1ccccc1C)C(C)=O\n"
     ]
    }
   ],
   "source": [
    "graph_test = test_dataset[2][1]\n",
    "from src.utils.my_utiles import graph2smiles\n",
    "smiles = graph2smiles(graph_test.edge_index, graph_test.edge_attr, graph_test.x)\n",
    "print(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['edge_index', 'edge_attr', 'x', 'num_nodes'])\n",
      "(2, 26)\n",
      "(26, 3)\n",
      "(13, 9)\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "from src.utils.my_utiles import smiles2graph\n",
    "smiles = \"CCO\"\n",
    "graph = smiles2graph(\"CC(=O)OC1=CC=CC=C1C(=O)O\")\n",
    "print(graph.keys())\n",
    "print(graph['edge_index'].shape)\n",
    "print(graph['edge_attr'].shape)\n",
    "print(graph['x'].shape)\n",
    "print(graph['num_nodes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 26])\n",
      "torch.Size([26, 3])\n",
      "torch.Size([13, 9])\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "from src.utils.my_utiles import graph_to_torch_geometric\n",
    "graph = graph_to_torch_geometric(graph)\n",
    "print(graph['edge_index'].shape)\n",
    "print(graph['edge_attr'].shape)\n",
    "print(graph['x'].shape)\n",
    "print(graph['num_nodes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC(=O)Oc1ccccc1C(=O)O\n"
     ]
    }
   ],
   "source": [
    "from src.utils.my_utiles import graph2smiles\n",
    "smiles = graph2smiles(graph.edge_index, graph.edge_attr, graph.x)\n",
    "print(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting tokenization results!\n",
      "Tokenize graph:\n",
      "Data(edge_index=[2, 32], edge_attr=[32, 3], x=[16, 9], y=[1], num_nodes=16, idx=2, idx_of_ds=0)\n",
      "[Warning] Set eos_idx to 100000000 for task pretrain!\n",
      "\n",
      "Tokens:\n",
      "[['50',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0',\n",
      "  'molecule#edge#1',\n",
      "  'molecule#edge#2'],\n",
      " ['51',\n",
      "  'molecule#node#0#6',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['52',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#4',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#2',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#2',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['53',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['54',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#2',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#1',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['53',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#1',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['52',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#4',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#2',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#2',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['51',\n",
      "  'molecule#node#0#6',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['55',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['56',\n",
      "  'molecule#node#0#7',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#1',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#1',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['55',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#1',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['57',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#4',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#3',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#2',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['55',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['51',\n",
      "  'molecule#node#0#6',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['50',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['58',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#1',\n",
      "  'molecule#edge#1#2',\n",
      "  'molecule#edge#2#1'],\n",
      " ['59',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['60',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['61',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['62',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['63',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['64',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['65',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#4',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#3',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#2',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['64',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['59',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1']]\n",
      "Labels:\n",
      "[['51',\n",
      "  'molecule#node#0#6',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['52',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#4',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#2',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#2',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['53',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['54',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#2',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#1',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['53',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#1',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['52',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#4',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#2',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#2',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['51',\n",
      "  'molecule#node#0#6',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['55',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['56',\n",
      "  'molecule#node#0#7',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#1',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#1',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['55',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#1',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['57',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#4',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#3',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#2',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['55',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['51',\n",
      "  'molecule#node#0#6',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['50',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['58',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#1',\n",
      "  'molecule#edge#1#2',\n",
      "  'molecule#edge#2#1'],\n",
      " ['59',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['60',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['61',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['62',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['63',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['64',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['65',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#4',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#3',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#2',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['64',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['59',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['<eos>',\n",
      "  '<eos>',\n",
      "  '<eos>',\n",
      "  '<eos>',\n",
      "  '<eos>',\n",
      "  '<eos>',\n",
      "  '<eos>',\n",
      "  '<eos>',\n",
      "  '<eos>',\n",
      "  '<eos>',\n",
      "  '<eos>',\n",
      "  '<eos>',\n",
      "  '<eos>']]\n",
      "embed:[]\n",
      "\n",
      "Inputs for model:\n",
      "{'attention_mask': [1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1],\n",
      " 'embed': array([], shape=(25, 0), dtype=float64),\n",
      " 'input_ids': [[72, 630, 685, 695, 709, 715, 724, 731, 736, 738, 740, 741, 742],\n",
      "               [73, 641, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "               [74, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [75, 630, 685, 695, 709, 715, 724, 731, 736, 738, 743, 748, 754],\n",
      "               [76, 630, 685, 695, 709, 716, 724, 731, 736, 738, 744, 748, 754],\n",
      "               [75, 630, 685, 695, 709, 715, 724, 731, 736, 738, 744, 748, 754],\n",
      "               [74, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [73, 641, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 754],\n",
      "               [77, 630, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "               [78, 652, 685, 691, 709, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "               [77, 630, 685, 695, 709, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "               [79, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [77, 630, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 754],\n",
      "               [73, 641, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "               [72, 630, 685, 695, 709, 715, 724, 731, 736, 738, 743, 748, 755],\n",
      "               [80, 630, 685, 695, 709, 715, 724, 731, 736, 738, 744, 750, 755],\n",
      "               [81, 630, 685, 695, 709, 714, 724, 731, 737, 739, 743, 748, 755],\n",
      "               [82, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "               [83, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "               [84, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "               [85, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "               [86, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "               [87, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [86, 630, 685, 695, 709, 714, 724, 731, 737, 739, 743, 748, 754],\n",
      "               [81,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755]],\n",
      " 'labels': [[73, 641, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "            [74, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [75, 630, 685, 695, 709, 715, 724, 731, 736, 738, 743, 748, 754],\n",
      "            [76, 630, 685, 695, 709, 716, 724, 731, 736, 738, 744, 748, 754],\n",
      "            [75, 630, 685, 695, 709, 715, 724, 731, 736, 738, 744, 748, 754],\n",
      "            [74, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [73, 641, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 754],\n",
      "            [77, 630, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "            [78, 652, 685, 691, 709, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "            [77, 630, 685, 695, 709, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "            [79, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [77, 630, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 754],\n",
      "            [73, 641, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "            [72, 630, 685, 695, 709, 715, 724, 731, 736, 738, 743, 748, 755],\n",
      "            [80, 630, 685, 695, 709, 715, 724, 731, 736, 738, 744, 750, 755],\n",
      "            [81, 630, 685, 695, 709, 714, 724, 731, 737, 739, 743, 748, 755],\n",
      "            [82, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [83, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [84, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [85, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [86, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [87, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [86, 630, 685, 695, 709, 714, 724, 731, 737, 739, 743, 748, 754],\n",
      "            [81, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19]],\n",
      " 'position_ids': [0,\n",
      "                  1,\n",
      "                  2,\n",
      "                  3,\n",
      "                  4,\n",
      "                  5,\n",
      "                  6,\n",
      "                  7,\n",
      "                  8,\n",
      "                  9,\n",
      "                  10,\n",
      "                  11,\n",
      "                  12,\n",
      "                  13,\n",
      "                  14,\n",
      "                  15,\n",
      "                  16,\n",
      "                  17,\n",
      "                  18,\n",
      "                  19,\n",
      "                  20,\n",
      "                  21,\n",
      "                  22,\n",
      "                  23,\n",
      "                  24]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_graph = graph_test\n",
    "from src.utils.my_utiles import graph2token2input\n",
    "import numpy as np\n",
    "token, label, embed, inputs = graph2token2input(example_graph, gtokenizer)\n",
    "\n",
    "print(\n",
    "    f\"\\nTokens:\\n{pformat(token)}\\nLabels:\\n{pformat(label)}\\nembed:{np.array(embed)}\\n\"\n",
    ")\n",
    "\n",
    "print(f\"Inputs for model:\\n{pformat(inputs)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'position_ids', 'labels', 'attention_mask', 'embed'])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'position_ids', 'labels', 'attention_mask', 'embed'])\n",
      "input_ids: torch.Size([1, 25, 13])\n",
      "position_ids: torch.Size([1, 25])\n",
      "labels: torch.Size([1, 25, 13])\n",
      "attention_mask: torch.Size([1, 25])\n",
      "embed: torch.Size([1, 25, 0])\n"
     ]
    }
   ],
   "source": [
    "from src.utils.my_utiles import convert_to_tensors\n",
    "\n",
    "tensor_inputs = convert_to_tensors(inputs)\n",
    "print(tensor_inputs.keys())\n",
    "\n",
    "print(\"input_ids:\", tensor_inputs[\"input_ids\"].shape)\n",
    "print(\"position_ids:\", tensor_inputs[\"position_ids\"].shape)\n",
    "print(\"labels:\", tensor_inputs[\"labels\"].shape)\n",
    "print(\"attention_mask:\", tensor_inputs[\"attention_mask\"].shape)\n",
    "print(\"embed:\", tensor_inputs[\"embed\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 72, 630, 685, 695, 709, 715, 724, 731, 736, 738, 740, 741, 742],\n",
      "         [ 73, 641, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "         [ 74, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "         [ 75, 630, 685, 695, 709, 715, 724, 731, 736, 738, 743, 748, 754],\n",
      "         [ 76, 630, 685, 695, 709, 716, 724, 731, 736, 738, 744, 748, 754],\n",
      "         [ 75, 630, 685, 695, 709, 715, 724, 731, 736, 738, 744, 748, 754],\n",
      "         [ 74, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "         [ 73, 641, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 754],\n",
      "         [ 77, 630, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "         [ 78, 652, 685, 691, 709, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "         [ 77, 630, 685, 695, 709, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "         [ 79, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754],\n",
      "         [ 77, 630, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 754],\n",
      "         [ 73, 641, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "         [ 72, 630, 685, 695, 709, 715, 724, 731, 736, 738, 743, 748, 755],\n",
      "         [ 80, 630, 685, 695, 709, 715, 724, 731, 736, 738, 744, 750, 755],\n",
      "         [ 81, 630, 685, 695, 709, 714, 724, 731, 737, 739, 743, 748, 755],\n",
      "         [ 82, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "         [ 83, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "         [ 84, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "         [ 85, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "         [ 86, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "         [ 87, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754],\n",
      "         [ 86, 630, 685, 695, 709, 714, 724, 731, 737, 739, 743, 748, 754],\n",
      "         [ 81, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755]]])\n",
      "tensor([[[ 73, 641, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "         [ 74, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "         [ 75, 630, 685, 695, 709, 715, 724, 731, 736, 738, 743, 748, 754],\n",
      "         [ 76, 630, 685, 695, 709, 716, 724, 731, 736, 738, 744, 748, 754],\n",
      "         [ 75, 630, 685, 695, 709, 715, 724, 731, 736, 738, 744, 748, 754],\n",
      "         [ 74, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "         [ 73, 641, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 754],\n",
      "         [ 77, 630, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "         [ 78, 652, 685, 691, 709, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "         [ 77, 630, 685, 695, 709, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "         [ 79, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754],\n",
      "         [ 77, 630, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 754],\n",
      "         [ 73, 641, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "         [ 72, 630, 685, 695, 709, 715, 724, 731, 736, 738, 743, 748, 755],\n",
      "         [ 80, 630, 685, 695, 709, 715, 724, 731, 736, 738, 744, 750, 755],\n",
      "         [ 81, 630, 685, 695, 709, 714, 724, 731, 737, 739, 743, 748, 755],\n",
      "         [ 82, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "         [ 83, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "         [ 84, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "         [ 85, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "         [ 86, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "         [ 87, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754],\n",
      "         [ 86, 630, 685, 695, 709, 714, 724, 731, 737, 739, 743, 748, 754],\n",
      "         [ 81, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "         [ 19,  19,  19,  19,  19,  19,  19,  19,  19,  19,  19,  19,  19]]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_inputs[\"input_ids\"])\n",
    "print(tensor_inputs[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 25, 13])\n",
      "position_ids: torch.Size([1, 25])\n",
      "labels: torch.Size([1, 25, 13])\n",
      "attention_mask: torch.Size([1, 25])\n",
      "embed: torch.Size([1, 25, 0])\n"
     ]
    }
   ],
   "source": [
    "print(\"input_ids:\", tensor_inputs[\"input_ids\"].shape)\n",
    "print(\"position_ids:\", tensor_inputs[\"position_ids\"].shape)\n",
    "print(\"labels:\", tensor_inputs[\"labels\"].shape)\n",
    "print(\"attention_mask:\", tensor_inputs[\"attention_mask\"].shape)\n",
    "print(\"embed:\", tensor_inputs[\"embed\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(1, 25, 0), dtype=torch.float64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_inputs[\"embed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28754\n",
      "Cc1ccc(C2Cc3cnccc3NC2=O)cc1\n",
      "Inspecting tokenization results!\n",
      "Tokenize graph:\n",
      "Data(edge_index=[2, 40], edge_attr=[40, 3], x=[18, 9], y=[1], num_nodes=18, idx=0, idx_of_ds=0)\n",
      "token (25, 13)\n",
      "label (25, 13)\n",
      "embed (25, 0)\n",
      "dict_keys(['input_ids', 'position_ids', 'labels', 'attention_mask', 'embed'])\n",
      "input_ids (25, 13)\n",
      "position_ids (25,)\n",
      "labels (25, 13)\n",
      "attention_mask (25,)\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from src.utils.my_utiles import graph2token2input_generation\n",
    "# smiles = \"CCO\" # \"CC(=O)OC1=CC=CC=C1C(=O)O\"\\\n",
    "random_index = np.random.randint(0, len(test_dataset))  # Upper bound is exclusive\n",
    "print(random_index)\n",
    "random_index = 0\n",
    "graph = test_dataset[random_index][1]\n",
    "# graph = graph_to_torch_geometric(graph)\n",
    "smiles2 = graph2smiles(graph.edge_index, graph.edge_attr, graph.x)\n",
    "example_graph = graph\n",
    "print(smiles2)\n",
    "\n",
    "num_input, max_length = 5, 40\n",
    "token, label, embed, inputs = graph2token2input(graph, gtokenizer)\n",
    "print('token', np.array(token).shape)\n",
    "print('label', np.array(label).shape)\n",
    "print('embed', np.array(embed).shape)\n",
    "print(inputs.keys())\n",
    "print('input_ids', np.array(inputs[\"input_ids\"]).shape)\n",
    "print('position_ids', np.array(inputs[\"position_ids\"]).shape)\n",
    "print('labels', np.array(inputs[\"labels\"]).shape)\n",
    "print('attention_mask', np.array(inputs[\"attention_mask\"]).shape)\n",
    "# print('embed', np.array(inputs[\"embed\"]).shape)\n",
    "print(inputs[\"attention_mask\"])\n",
    "tensor_inputs = convert_to_tensors(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---\n",
      "input_embeds None\n",
      "input_ids at beginning torch.Size([1, 25, 13])\n",
      "inputs_embeds torch.Size([1, 25, 512])\n",
      "attention_mask torch.Size([1, 25])\n",
      "odict_keys(['last_hidden_state'])\n",
      "torch.Size([1, 25, 512])\n",
      "labels torch.Size([1, 25, 13])\n",
      "labels_m torch.Size([1, 25])\n",
      "mask_m torch.Size([1, 25])\n",
      "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True]], device='cuda:0')\n",
      "hidden_states torch.Size([25, 512])\n",
      "hidden_states torch.Size([25, 6656])\n",
      "hidden_states torch.Size([325, 512])\n",
      "logits torch.Size([325, 756])\n",
      "end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---\n",
      "CausalLMOutputWithPast(loss=tensor(0.1329, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-1.0674, -0.8618, -1.1148,  ..., -1.0357, -0.4052,  0.4758],\n",
      "        [-8.1995, -8.0857, -8.1216,  ..., -8.0170, -3.0487, -2.8547],\n",
      "        [-4.4565, -4.8677, -4.7255,  ..., -4.8237, -0.9753, -1.1261],\n",
      "        ...,\n",
      "        [ 0.3238,  0.6337,  0.0745,  ..., -0.1129,  2.0087,  0.2192],\n",
      "        [ 2.5266,  2.9248,  3.1552,  ...,  3.0035,  3.3000,  0.6159],\n",
      "        [-0.5597, -0.6561, -0.2994,  ..., -0.9540,  3.7521,  3.2108]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>), past_key_values=None, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "data = tensor_inputs\n",
    "input_ids = data[\"input_ids\"].to(device)\n",
    "# print(input_ids)\n",
    "attention_mask = data[\"attention_mask\"].to(device)\n",
    "labels = data[\"labels\"].to(device)\n",
    "inputs_raw_embeds = None\n",
    "if embed_dim > 0: # in tokenizer config\n",
    "    inputs_raw_embeds = data[\"embed\"].to(device)\n",
    "print(inputs_raw_embeds)\n",
    "output = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    labels=labels,\n",
    "    inputs_raw_embeds=inputs_raw_embeds,\n",
    ")  # Perform a single forward pass.\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"labels:\", tensor_inputs[\"labels\"].shape)\n",
    "# print(\"output:\", output.keys())\n",
    "# print(\"output:\", output[\"logits\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_labels = torch.argmax(output[\"logits\"], dim=-1) \n",
    "# reshaped_labels = predicted_labels.view(*tensor_inputs[\"labels\"].shape)\n",
    "# print(\"labels:\", tensor_inputs[\"labels\"])\n",
    "# print(\"predicted_labels:\", reshaped_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # File path to your vocabulary file\n",
    "# vocab_file_path = \"/datalake/datastore1/yang/graph-gpt/data/OGB/pcqm4m-v2/vocab512_stacked\"\n",
    "\n",
    "# from src.utils.my_utiles import load_vocab\n",
    "\n",
    "# vocab = load_vocab(vocab_file_path)\n",
    "\n",
    "# from src.utils.my_utiles import convert_labels_to_tokens\n",
    "# # Example usage\n",
    "# # Assuming `reshaped_labels` contains the predicted label IDs of shape [1, 24, 13]\n",
    "# tokens = convert_labels_to_tokens(reshaped_labels, vocab)\n",
    "\n",
    "# # Optional: Reshape tokens back to the original structure for visualization\n",
    "# tokens_reshaped = [\n",
    "#     [tokens[i * 13 + j] for j in range(13)] for i in range(tensor_inputs[\"labels\"].shape[1])\n",
    "# ]\n",
    "\n",
    "# print(reshaped_labels.shape)\n",
    "# # Print tokens\n",
    "# print(np.array(tokens_reshaped).shape)\n",
    "# pprint(tokens_reshaped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29082\n",
      "Cc1ccc(C2Cc3cnccc3NC2=O)cc1\n",
      "Inspecting tokenization results!\n",
      "Tokenize graph:\n",
      "Data(edge_index=[2, 40], edge_attr=[40, 3], x=[18, 9], y=[1], num_nodes=18, idx=0, idx_of_ds=0)\n",
      "token (5, 13)\n",
      "label (25, 13)\n",
      "embed (5, 0)\n",
      "dict_keys(['input_ids', 'position_ids', 'labels', 'attention_mask', 'embed'])\n",
      "input_ids (5, 13)\n",
      "position_ids (5,)\n",
      "labels (25, 13)\n",
      "attention_mask (5,)\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from src.utils.my_utiles import graph2token2input_generation\n",
    "# smiles = \"CCO\" # \"CC(=O)OC1=CC=CC=C1C(=O)O\"\\\n",
    "random_index = np.random.randint(0, len(test_dataset))  # Upper bound is exclusive\n",
    "print(random_index)\n",
    "random_index = 0\n",
    "graph = test_dataset[random_index][1]\n",
    "# graph = graph_to_torch_geometric(graph)\n",
    "smiles2 = graph2smiles(graph.edge_index, graph.edge_attr, graph.x)\n",
    "example_graph = graph\n",
    "print(smiles2)\n",
    "\n",
    "num_input, max_length = 5, 40\n",
    "token, label, embed, inputs = graph2token2input_generation(graph, gtokenizer, num_input, max_length)\n",
    "print('token', np.array(token).shape)\n",
    "print('label', np.array(label).shape)\n",
    "print('embed', np.array(embed).shape)\n",
    "print(inputs.keys())\n",
    "print('input_ids', np.array(inputs[\"input_ids\"]).shape)\n",
    "print('position_ids', np.array(inputs[\"position_ids\"]).shape)\n",
    "print('labels', np.array(inputs[\"labels\"]).shape)\n",
    "print('attention_mask', np.array(inputs[\"attention_mask\"]).shape)\n",
    "# print('embed', np.array(inputs[\"embed\"]).shape)\n",
    "print(inputs[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(token)\n",
    "# pprint(label)\n",
    "# pprint(inputs[\"input_ids\"])\n",
    "# pprint(inputs[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_inputs = convert_to_tensors(inputs)\n",
    "# print(tensor_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'position_ids', 'labels', 'attention_mask', 'embed'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---\n",
      "input_embeds None\n",
      "input_ids at beginning torch.Size([1, 5, 13])\n",
      "inputs_embeds torch.Size([1, 5, 512])\n",
      "attention_mask torch.Size([1, 5])\n",
      "odict_keys(['last_hidden_state', 'past_key_values'])\n",
      "torch.Size([1, 5, 512])\n",
      "tensor([[True, True, True, True, True]])\n",
      "hidden_states torch.Size([1, 5, 512])\n",
      "hidden_states torch.Size([5, 512])\n",
      "hidden_states torch.Size([5, 6656])\n",
      "hidden_states torch.Size([65, 512])\n",
      "logits torch.Size([1, 65, 756])\n",
      "end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---end model ---\n",
      "begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---begin sample---\n",
      "next_token_logits.shape:  torch.Size([1, 13, 756])\n",
      "probs.shape:  torch.Size([1, 13, 756])\n",
      "next_tokens.shape:  torch.Size([1, 13])\n",
      "input_ids.shape:  torch.Size([1, 6, 13])\n",
      "end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---end sample---\n",
      "begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---begin model ---\n",
      "input_embeds None\n",
      "input_ids at beginning torch.Size([1, 1, 13])\n",
      "inputs_embeds torch.Size([1, 1, 512])\n",
      "attention_mask torch.Size([1, 6])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 2\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# Input token IDs\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass the attention mask\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mposition_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Pass the position IDs\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# Maximum generation length\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Enable sampling\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Temperature for randomness\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/graph_gpt/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/graph_gpt/lib/python3.8/site-packages/transformers/generation/utils.py:1592\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1585\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1586\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1587\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1589\u001b[0m     )\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1610\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1611\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1616\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1617\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/graph_gpt/lib/python3.8/site-packages/transformers/generation/utils.py:2696\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2695\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2696\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2697\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2699\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2700\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2701\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2704\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/graph_gpt/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/graph_gpt/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/datalake/datastore1/yang/graph-gpt/./src/models/graphgpt/modeling_graphgpt.py:262\u001b[0m, in \u001b[0;36mGraphGPTCausal.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, eulerian_position_ids, past_key_values, inputs_embeds, inputs_raw_embeds, labels, label_mask, prior, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# print(\"position_ids\", position_ids.shape)\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m    263\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    264\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    265\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    273\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "output_ids = model.generate(\n",
    "    input_ids=tensor_inputs[\"input_ids\"].to(device) ,          # Input token IDs\n",
    "    attention_mask=tensor_inputs[\"attention_mask\"].to(device),  # Pass the attention mask\n",
    "    position_ids=tensor_inputs[\"position_ids\"].to(device),      # Pass the position IDs\n",
    "    max_length=20,                # Maximum generation length\n",
    "    do_sample=True,               # Enable sampling\n",
    "    temperature=0.7               # Temperature for randomness\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
