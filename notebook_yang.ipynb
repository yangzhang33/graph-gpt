{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import fire\n",
    "import copy\n",
    "import multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.cuda.amp import GradScaler\n",
    "import deepspeed\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "from pprint import pprint, pformat\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from timm.utils import ModelEmaV3\n",
    "from timm.models import load_checkpoint\n",
    "from timm.utils.model import unwrap_model, get_state_dict\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ModuleNotFoundError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "import sys\n",
    "\n",
    "# sys.path.insert(0, \"..\")\n",
    "sys.path.insert(0, \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import (\n",
    "    collator,\n",
    "    vocab_builder,\n",
    "    tokenizer,\n",
    "    read_dataset,\n",
    "    OdpsTableIterableDataset,\n",
    ")\n",
    "from src.models import (\n",
    "    GraphGPTConfig,\n",
    "    GraphGPTCausal,\n",
    "    GraphGPT2Config,\n",
    "    GraphGPT2Causal,\n",
    "    GraphBertConfig,\n",
    "    GraphBertForMaskedLM,\n",
    ")\n",
    "from src.utils import (\n",
    "    conf_utils,\n",
    "    loss_utils,\n",
    "    loader_utils,\n",
    "    tokenizer_utils,\n",
    "    modules_utils,\n",
    "    misc_utils,\n",
    "    print_trainable_parameters,\n",
    "    print_params,\n",
    "    inspect_tokenization_results,\n",
    "    set_up_shuffle_and_sampler,\n",
    "    worker_init_fn_seed,\n",
    ")\n",
    "\n",
    "dict_models = {\n",
    "    \"graphgpt2\": (GraphGPT2Causal, GraphGPT2Config),\n",
    "    \"graphgpt\": (GraphGPTCausal, GraphGPTConfig),\n",
    "    \"graphbert\": (GraphBertForMaskedLM, GraphBertConfig),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir: str = \"../data/TUDataset\"\n",
    "tables: str = \"\"\n",
    "deepspeed_config = \"./examples/ds_config2_pt.json\"\n",
    "intermediate_size = 0\n",
    "num_attention_heads = 0\n",
    "hidden_size = 512\n",
    "num_hidden_layers = 8\n",
    "task_type='pretrain'\n",
    "causal_attention = 1\n",
    "lr=3e-4\n",
    "model_type = 'graphgpt'\n",
    "output_dir='./exp/models/pcqm4m-v2/test'\n",
    "pretrain_cpt = ''\n",
    "samples_per_saving=1000000\n",
    "\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m     pretrain_cpt \u001b[38;5;241m=\u001b[39m output_dir\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# 0. init distributed train and get gpu/device info\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_process_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnccl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menv://\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# for distributed training\u001b[39;00m\n\u001b[1;32m     39\u001b[0m dist\u001b[38;5;241m.\u001b[39mbarrier() \u001b[38;5;66;03m# for sync training\u001b[39;00m\n\u001b[1;32m     40\u001b[0m world_size \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mget_world_size() \u001b[38;5;66;03m# 1 # number of GPUs\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/graph_gpt/lib/python3.8/site-packages/torch/distributed/c10d_logger.py:86\u001b[0m, in \u001b[0;36m_time_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     85\u001b[0m     t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns()\n\u001b[0;32m---> 86\u001b[0m     func_return \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns() \u001b[38;5;241m-\u001b[39m t1\n\u001b[1;32m     89\u001b[0m     msg_dict \u001b[38;5;241m=\u001b[39m _get_msg_dict(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/graph_gpt/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:1177\u001b[0m, in \u001b[0;36minit_process_group\u001b[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1174\u001b[0m     rendezvous_iterator \u001b[38;5;241m=\u001b[39m rendezvous(\n\u001b[1;32m   1175\u001b[0m         init_method, rank, world_size, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1176\u001b[0m     )\n\u001b[0;32m-> 1177\u001b[0m     store, rank, world_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrendezvous_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1178\u001b[0m     store\u001b[38;5;241m.\u001b[39mset_timeout(timeout)\n\u001b[1;32m   1180\u001b[0m     \u001b[38;5;66;03m# Use a PrefixStore to avoid accidental overrides of keys used by\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;66;03m# different systems (e.g. RPC) in case the store is multi-tenant.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/graph_gpt/lib/python3.8/site-packages/torch/distributed/rendezvous.py:234\u001b[0m, in \u001b[0;36m_env_rendezvous_handler\u001b[0;34m(url, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m     rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(query_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m     rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43m_get_env_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRANK\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworld_size\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m query_dict:\n\u001b[1;32m    237\u001b[0m     world_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(query_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworld_size\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/graph_gpt/lib/python3.8/site-packages/torch/distributed/rendezvous.py:219\u001b[0m, in \u001b[0;36m_env_rendezvous_handler.<locals>._get_env_or_raise\u001b[0;34m(env_var)\u001b[0m\n\u001b[1;32m    217\u001b[0m env_val \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(env_var, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_val:\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _env_error(env_var)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_val\n",
      "\u001b[0;31mValueError\u001b[0m: Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set"
     ]
    }
   ],
   "source": [
    "use_tb_writer = False           # use tensorboard writer\n",
    "use_ema = False # False # use exponential moving average to smooth model\n",
    "ema_file = \"model_ema.pt\"\n",
    "ema_file_best = \"model_ema_best.pt\"\n",
    "ema_best_res = None\n",
    "ema_best_flag = False\n",
    "use_deepspeed = len(deepspeed_config) > 0 # True # use deepspeed for training, good to set scheduler\n",
    "if use_ema:\n",
    "    do_test = 1\n",
    "if (intermediate_size == 0) and (num_attention_heads == 0): # True\n",
    "    (\n",
    "        hidden_size,\n",
    "        intermediate_size,\n",
    "        num_attention_heads,\n",
    "        num_hidden_layers,\n",
    "    ) = modules_utils.set_up_model_architect(\n",
    "        hidden_size=hidden_size, num_hidden_layers=num_hidden_layers # 768 24 related to model names intermediate_size = hidden_size * 4, num_attention_heads = hidden_size // 64\n",
    "    )# 768 3072 12 24\n",
    "causal_attention = 0 if task_type == \"pretrain-mlm\" else causal_attention\n",
    "#########################\n",
    "betas = (0.9, 0.95) # used in AdamW optimizer, important for config beta\n",
    "#########################\n",
    "# lr * 0.1 -> from llama2 pre-train settings\n",
    "min_lr = lr * 0.1 if use_deepspeed else 0    # used in scheduler, when not using deepspeed.\n",
    "#########################\n",
    "gpu_name = torch.cuda.get_device_name()\n",
    "GraphModel, GraphModelConfig = dict_models[model_type] # Not instantiate yet\n",
    "\n",
    "\n",
    "if os.path.exists(os.path.join(output_dir, \"log.csv\")):\n",
    "    print(\n",
    "        f\"log file {os.path.join(output_dir, 'log.csv')} exists, resume training from {output_dir} instead of initializing from pre-train ckp {pretrain_cpt}!\"\n",
    "    )\n",
    "    pretrain_cpt = output_dir\n",
    "\n",
    "\n",
    "# 0. init distributed train and get gpu/device info\n",
    "dist.init_process_group(backend=\"nccl\", init_method=\"env://\")  # for distributed training\n",
    "dist.barrier() # for sync training\n",
    "world_size = dist.get_world_size() # 1 # number of GPUs\n",
    "rank = dist.get_rank() # 0 # current GPU index\n",
    "local_rank = os.environ.get(\"LOCAL_RANK\") # 0 # current GPU index local to the node\n",
    "print(f\"\\nworld size: {world_size}, rank: {rank}, local rank: {local_rank}\") # 1 0 0\n",
    "rnd_seed = torch.random.initial_seed() - rank\n",
    "random.seed(rnd_seed)\n",
    "print(f\"seed random with {rnd_seed}\") # 1234\n",
    "steps_per_saving = samples_per_saving // (world_size * batch_size) # 1000000 // (1 * 1024) = 976\n",
    "print(f\"\\nsteps_per_saving: {steps_per_saving}\") # 976\n",
    "params = print_params(**locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer config loading\n",
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "file_path = \"./zhang_test/tokenizer_config.json\"\n",
    "with open(file_path, \"r\") as json_file:\n",
    "    tokenizer_config = json.load(json_file)\n",
    "\n",
    "# Print the loaded data\n",
    "pprint(tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset PCQM4Mv2 ...\n",
      "\n",
      "dataset._data -> Data(edge_index=[2, 109093626], edge_attr=[109093626, 3], x=[52970652, 9], y=[3746620])\n",
      "In pre-train mode, set all valid data's y to nan!\n",
      "Before setting, y has 294469 NANs\n",
      "After setting, y has 368014 NANs\n",
      "Default process group has not been initialized, please make sure to call init_process_group.\n",
      "\n",
      "Raw indices: 3746620, Removed indices: 0, New indices: 3746620\n",
      "\n",
      "Raw indices: 3746620, Removed indices: 294469, New indices: 3452151\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "\n",
      "[2024-12-09 01:28:00.336220] NOT RESET samples of GraphsMapDataset of 3452151 graphs for epoch None!\n",
      "idx_tuple: None\n",
      "(0, Data(edge_index=[2, 40], edge_attr=[40, 3], x=[18, 9], y=[1, 1], num_nodes=18, idx=0, idx_of_ds=0))\n"
     ]
    }
   ],
   "source": [
    "# 1.2 get graph dataset\n",
    "dataset, raw_dataset = read_dataset(\n",
    "    name=tokenizer_config[\"dataset\"],   # PCQM4Mv2\n",
    "    # for local data file reading\n",
    "    data_dir=data_dir,   # './data/OGB'\n",
    "    sampling_config=tokenizer_config[\"sampling\"],    # None\n",
    "    # for odps data reading\n",
    "    table=tables,   # \"\"\n",
    "    edge_dim=tokenizer_config[\"semantics\"][\"edge\"][\"dim\"],    # 3\n",
    "    node_dim=tokenizer_config[\"semantics\"][\"node\"][\"dim\"],    # 9\n",
    "    mode=\"train\",\n",
    "    # general\n",
    "    pretrain_mode=True,\n",
    "    ensemble_datasets=tokenizer_config.get(\"ensemble_datasets\", []),    # []\n",
    ")\n",
    "reset_samples_per_epoch = (   # what is this  # None for PCQM4Mv2\n",
    "    dataset.reset_samples_per_epoch\n",
    "    if hasattr(dataset, \"reset_samples_per_epoch\")\n",
    "    else False\n",
    ")\n",
    "if isinstance(dataset, IterableDataset):\n",
    "    print(next(iter(dataset))) \n",
    "else: # True\n",
    "    idx = dataset.sampler[0] # (0, Data(edge_index=[2, 40], edge_attr=[40, 3], x=[18, 9], y=[1, 1], num_nodes=18, idx=0, idx_of_ds=0))\n",
    "    print(dataset[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<src.data.dataset_map.GraphsMapDataset object at 0x7fa029091280>\n",
      "length of dataset: 3452151\n",
      "(1, Data(edge_index=[2, 34], edge_attr=[34, 3], x=[17, 9], y=[1, 1], num_nodes=17, idx=1, idx_of_ds=0))\n",
      "(2, Data(edge_index=[2, 32], edge_attr=[32, 3], x=[16, 9], y=[1, 1], num_nodes=16, idx=2, idx_of_ds=0))\n",
      "####################################################################################################\n",
      "example\n",
      "edge_index:  tensor([[16, 15, 15, 13, 13,  7,  7,  2,  2, 10, 10,  9,  2,  1,  1,  8,  8,  5,\n",
      "          5,  4,  4,  6,  6, 11, 11, 12, 12,  0, 12, 14, 11,  3,  5, 13],\n",
      "        [15, 16, 13, 15,  7, 13,  2,  7, 10,  2,  9, 10,  1,  2,  8,  1,  5,  8,\n",
      "          4,  5,  6,  4, 11,  6, 12, 11,  0, 12, 14, 12,  3, 11, 13,  5]])\n",
      "edge_attr:  tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [1, 2, 1],\n",
      "        [1, 2, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [1, 0, 1],\n",
      "        [1, 0, 1],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1]])\n",
      "x:  tensor([[5, 0, 3, 5, 1, 0, 1, 1, 1],\n",
      "        [5, 0, 3, 5, 1, 0, 1, 1, 1],\n",
      "        [5, 0, 4, 5, 3, 0, 2, 0, 0],\n",
      "        [5, 0, 4, 5, 3, 0, 2, 0, 0],\n",
      "        [5, 0, 4, 5, 3, 0, 2, 0, 0],\n",
      "        [7, 0, 2, 5, 0, 0, 1, 0, 0],\n",
      "        [6, 0, 3, 5, 0, 0, 1, 0, 0],\n",
      "        [7, 0, 1, 5, 0, 0, 1, 0, 0],\n",
      "        [5, 0, 3, 5, 0, 0, 1, 0, 0],\n",
      "        [5, 0, 3, 5, 1, 0, 1, 0, 0],\n",
      "        [7, 0, 2, 5, 0, 0, 1, 0, 0],\n",
      "        [5, 0, 3, 5, 0, 0, 1, 1, 1],\n",
      "        [5, 0, 3, 5, 1, 0, 1, 1, 1],\n",
      "        [5, 0, 3, 5, 0, 0, 1, 1, 1],\n",
      "        [5, 0, 3, 5, 0, 0, 1, 1, 1],\n",
      "        [5, 0, 4, 5, 3, 0, 2, 0, 0],\n",
      "        [5, 0, 3, 5, 1, 0, 1, 0, 0]])\n",
      "y:  tensor([[4.4110]])\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print('length of dataset:', len(dataset))\n",
    "print(dataset[1])\n",
    "print(dataset[2])\n",
    "print(\"#\" * 100)\n",
    "print(\"example\")\n",
    "print(\"edge_index: \", dataset[1][1].edge_index)\n",
    "print(\"edge_attr: \", dataset[1][1].edge_attr)\n",
    "print(\"x: \", dataset[1][1].x)\n",
    "print(\"y: \", dataset[1][1].y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-09 01:28:00.409230] Vocab is already built and saved in ./data/OGB/pcqm4m-v2/vocab512_stacked!\n",
      "[2024-12-09 01:28:00.409632] Loading vocab from ./data/OGB/pcqm4m-v2/vocab512_stacked ...\n",
      "[2024-12-09 01:28:00.411244]\n",
      "{   '0': 22,\n",
      "    '1': 23,\n",
      "    '10': 32,\n",
      "    '100': 122,\n",
      "    '101': 123,\n",
      "    '102': 124,\n",
      "    '103': 125,\n",
      "    '104': 126,\n",
      "    '105': 127,\n",
      "    '106': 128,\n",
      "    '107': 129,\n",
      "    '108': 130,\n",
      "    '109': 131,\n",
      "    '11': 33,\n",
      "    '110': 132,\n",
      "    '111': 133,\n",
      "    '112': 134,\n",
      "    '113': 135,\n",
      "    '114': 136,\n",
      "    '115': 137,\n",
      "    '116': 138,\n",
      "    '117': 139,\n",
      "    '118': 140,\n",
      "    '119': 141,\n",
      "    '12': 34,\n",
      "    '120': 142,\n",
      "    '121': 143,\n",
      "    '122': 144,\n",
      "    '123': 145,\n",
      "    '124': 146,\n",
      "    '125': 147,\n",
      "    '126': 148,\n",
      "    '127': 149,\n",
      "    '128': 150,\n",
      "    '129': 151,\n",
      "    '13': 35,\n",
      "    '130': 152,\n",
      "    '131': 153,\n",
      "    '132': 154,\n",
      "    '133': 155,\n",
      "    '134': 156,\n",
      "    '135': 157,\n",
      "    '136': 158,\n",
      "    '137': 159,\n",
      "    '138': 160,\n",
      "    '139': 161,\n",
      "    '14': 36,\n",
      "    '140': 162,\n",
      "    '141': 163,\n",
      "    '142': 164,\n",
      "    '143': 165,\n",
      "    '144': 166,\n",
      "    '145': 167,\n",
      "    '146': 168,\n",
      "    '147': 169,\n",
      "    '148': 170,\n",
      "    '149': 171,\n",
      "    '15': 37,\n",
      "    '150': 172,\n",
      "    '151': 173,\n",
      "    '152': 174,\n",
      "    '153': 175,\n",
      "    '154': 176,\n",
      "    '155': 177,\n",
      "    '156': 178,\n",
      "    '157': 179,\n",
      "    '158': 180,\n",
      "    '159': 181,\n",
      "    '16': 38,\n",
      "    '160': 182,\n",
      "    '161': 183,\n",
      "    '162': 184,\n",
      "    '163': 185,\n",
      "    '164': 186,\n",
      "    '165': 187,\n",
      "    '166': 188,\n",
      "    '167': 189,\n",
      "    '168': 190,\n",
      "    '169': 191,\n",
      "    '17': 39,\n",
      "    '170': 192,\n",
      "    '171': 193,\n",
      "    '172': 194,\n",
      "    '173': 195,\n",
      "    '174': 196,\n",
      "    '175': 197,\n",
      "    '176': 198,\n",
      "    '177': 199,\n",
      "    '178': 200,\n",
      "    '179': 201,\n",
      "    '18': 40,\n",
      "    '180': 202,\n",
      "    '181': 203,\n",
      "    '182': 204,\n",
      "    '183': 205,\n",
      "    '184': 206,\n",
      "    '185': 207,\n",
      "    '186': 208,\n",
      "    '187': 209,\n",
      "    '188': 210,\n",
      "    '189': 211,\n",
      "    '19': 41,\n",
      "    '190': 212,\n",
      "    '191': 213,\n",
      "    '192': 214,\n",
      "    '193': 215,\n",
      "    '194': 216,\n",
      "    '195': 217,\n",
      "    '196': 218,\n",
      "    '197': 219,\n",
      "    '198': 220,\n",
      "    '199': 221,\n",
      "    '2': 24,\n",
      "    '20': 42,\n",
      "    '200': 222,\n",
      "    '201': 223,\n",
      "    '202': 224,\n",
      "    '203': 225,\n",
      "    '204': 226,\n",
      "    '205': 227,\n",
      "    '206': 228,\n",
      "    '207': 229,\n",
      "    '208': 230,\n",
      "    '209': 231,\n",
      "    '21': 43,\n",
      "    '210': 232,\n",
      "    '211': 233,\n",
      "    '212': 234,\n",
      "    '213': 235,\n",
      "    '214': 236,\n",
      "    '215': 237,\n",
      "    '216': 238,\n",
      "    '217': 239,\n",
      "    '218': 240,\n",
      "    '219': 241,\n",
      "    '22': 44,\n",
      "    '220': 242,\n",
      "    '221': 243,\n",
      "    '222': 244,\n",
      "    '223': 245,\n",
      "    '224': 246,\n",
      "    '225': 247,\n",
      "    '226': 248,\n",
      "    '227': 249,\n",
      "    '228': 250,\n",
      "    '229': 251,\n",
      "    '23': 45,\n",
      "    '230': 252,\n",
      "    '231': 253,\n",
      "    '232': 254,\n",
      "    '233': 255,\n",
      "    '234': 256,\n",
      "    '235': 257,\n",
      "    '236': 258,\n",
      "    '237': 259,\n",
      "    '238': 260,\n",
      "    '239': 261,\n",
      "    '24': 46,\n",
      "    '240': 262,\n",
      "    '241': 263,\n",
      "    '242': 264,\n",
      "    '243': 265,\n",
      "    '244': 266,\n",
      "    '245': 267,\n",
      "    '246': 268,\n",
      "    '247': 269,\n",
      "    '248': 270,\n",
      "    '249': 271,\n",
      "    '25': 47,\n",
      "    '250': 272,\n",
      "    '251': 273,\n",
      "    '252': 274,\n",
      "    '253': 275,\n",
      "    '254': 276,\n",
      "    '255': 277,\n",
      "    '256': 278,\n",
      "    '257': 279,\n",
      "    '258': 280,\n",
      "    '259': 281,\n",
      "    '26': 48,\n",
      "    '260': 282,\n",
      "    '261': 283,\n",
      "    '262': 284,\n",
      "    '263': 285,\n",
      "    '264': 286,\n",
      "    '265': 287,\n",
      "    '266': 288,\n",
      "    '267': 289,\n",
      "    '268': 290,\n",
      "    '269': 291,\n",
      "    '27': 49,\n",
      "    '270': 292,\n",
      "    '271': 293,\n",
      "    '272': 294,\n",
      "    '273': 295,\n",
      "    '274': 296,\n",
      "    '275': 297,\n",
      "    '276': 298,\n",
      "    '277': 299,\n",
      "    '278': 300,\n",
      "    '279': 301,\n",
      "    '28': 50,\n",
      "    '280': 302,\n",
      "    '281': 303,\n",
      "    '282': 304,\n",
      "    '283': 305,\n",
      "    '284': 306,\n",
      "    '285': 307,\n",
      "    '286': 308,\n",
      "    '287': 309,\n",
      "    '288': 310,\n",
      "    '289': 311,\n",
      "    '29': 51,\n",
      "    '290': 312,\n",
      "    '291': 313,\n",
      "    '292': 314,\n",
      "    '293': 315,\n",
      "    '294': 316,\n",
      "    '295': 317,\n",
      "    '296': 318,\n",
      "    '297': 319,\n",
      "    '298': 320,\n",
      "    '299': 321,\n",
      "    '3': 25,\n",
      "    '30': 52,\n",
      "    '300': 322,\n",
      "    '301': 323,\n",
      "    '302': 324,\n",
      "    '303': 325,\n",
      "    '304': 326,\n",
      "    '305': 327,\n",
      "    '306': 328,\n",
      "    '307': 329,\n",
      "    '308': 330,\n",
      "    '309': 331,\n",
      "    '31': 53,\n",
      "    '310': 332,\n",
      "    '311': 333,\n",
      "    '312': 334,\n",
      "    '313': 335,\n",
      "    '314': 336,\n",
      "    '315': 337,\n",
      "    '316': 338,\n",
      "    '317': 339,\n",
      "    '318': 340,\n",
      "    '319': 341,\n",
      "    '32': 54,\n",
      "    '320': 342,\n",
      "    '321': 343,\n",
      "    '322': 344,\n",
      "    '323': 345,\n",
      "    '324': 346,\n",
      "    '325': 347,\n",
      "    '326': 348,\n",
      "    '327': 349,\n",
      "    '328': 350,\n",
      "    '329': 351,\n",
      "    '33': 55,\n",
      "    '330': 352,\n",
      "    '331': 353,\n",
      "    '332': 354,\n",
      "    '333': 355,\n",
      "    '334': 356,\n",
      "    '335': 357,\n",
      "    '336': 358,\n",
      "    '337': 359,\n",
      "    '338': 360,\n",
      "    '339': 361,\n",
      "    '34': 56,\n",
      "    '340': 362,\n",
      "    '341': 363,\n",
      "    '342': 364,\n",
      "    '343': 365,\n",
      "    '344': 366,\n",
      "    '345': 367,\n",
      "    '346': 368,\n",
      "    '347': 369,\n",
      "    '348': 370,\n",
      "    '349': 371,\n",
      "    '35': 57,\n",
      "    '350': 372,\n",
      "    '351': 373,\n",
      "    '352': 374,\n",
      "    '353': 375,\n",
      "    '354': 376,\n",
      "    '355': 377,\n",
      "    '356': 378,\n",
      "    '357': 379,\n",
      "    '358': 380,\n",
      "    '359': 381,\n",
      "    '36': 58,\n",
      "    '360': 382,\n",
      "    '361': 383,\n",
      "    '362': 384,\n",
      "    '363': 385,\n",
      "    '364': 386,\n",
      "    '365': 387,\n",
      "    '366': 388,\n",
      "    '367': 389,\n",
      "    '368': 390,\n",
      "    '369': 391,\n",
      "    '37': 59,\n",
      "    '370': 392,\n",
      "    '371': 393,\n",
      "    '372': 394,\n",
      "    '373': 395,\n",
      "    '374': 396,\n",
      "    '375': 397,\n",
      "    '376': 398,\n",
      "    '377': 399,\n",
      "    '378': 400,\n",
      "    '379': 401,\n",
      "    '38': 60,\n",
      "    '380': 402,\n",
      "    '381': 403,\n",
      "    '382': 404,\n",
      "    '383': 405,\n",
      "    '384': 406,\n",
      "    '385': 407,\n",
      "    '386': 408,\n",
      "    '387': 409,\n",
      "    '388': 410,\n",
      "    '389': 411,\n",
      "    '39': 61,\n",
      "    '390': 412,\n",
      "    '391': 413,\n",
      "    '392': 414,\n",
      "    '393': 415,\n",
      "    '394': 416,\n",
      "    '395': 417,\n",
      "    '396': 418,\n",
      "    '397': 419,\n",
      "    '398': 420,\n",
      "    '399': 421,\n",
      "    '4': 26,\n",
      "    '40': 62,\n",
      "    '400': 422,\n",
      "    '401': 423,\n",
      "    '402': 424,\n",
      "    '403': 425,\n",
      "    '404': 426,\n",
      "    '405': 427,\n",
      "    '406': 428,\n",
      "    '407': 429,\n",
      "    '408': 430,\n",
      "    '409': 431,\n",
      "    '41': 63,\n",
      "    '410': 432,\n",
      "    '411': 433,\n",
      "    '412': 434,\n",
      "    '413': 435,\n",
      "    '414': 436,\n",
      "    '415': 437,\n",
      "    '416': 438,\n",
      "    '417': 439,\n",
      "    '418': 440,\n",
      "    '419': 441,\n",
      "    '42': 64,\n",
      "    '420': 442,\n",
      "    '421': 443,\n",
      "    '422': 444,\n",
      "    '423': 445,\n",
      "    '424': 446,\n",
      "    '425': 447,\n",
      "    '426': 448,\n",
      "    '427': 449,\n",
      "    '428': 450,\n",
      "    '429': 451,\n",
      "    '43': 65,\n",
      "    '430': 452,\n",
      "    '431': 453,\n",
      "    '432': 454,\n",
      "    '433': 455,\n",
      "    '434': 456,\n",
      "    '435': 457,\n",
      "    '436': 458,\n",
      "    '437': 459,\n",
      "    '438': 460,\n",
      "    '439': 461,\n",
      "    '44': 66,\n",
      "    '440': 462,\n",
      "    '441': 463,\n",
      "    '442': 464,\n",
      "    '443': 465,\n",
      "    '444': 466,\n",
      "    '445': 467,\n",
      "    '446': 468,\n",
      "    '447': 469,\n",
      "    '448': 470,\n",
      "    '449': 471,\n",
      "    '45': 67,\n",
      "    '450': 472,\n",
      "    '451': 473,\n",
      "    '452': 474,\n",
      "    '453': 475,\n",
      "    '454': 476,\n",
      "    '455': 477,\n",
      "    '456': 478,\n",
      "    '457': 479,\n",
      "    '458': 480,\n",
      "    '459': 481,\n",
      "    '46': 68,\n",
      "    '460': 482,\n",
      "    '461': 483,\n",
      "    '462': 484,\n",
      "    '463': 485,\n",
      "    '464': 486,\n",
      "    '465': 487,\n",
      "    '466': 488,\n",
      "    '467': 489,\n",
      "    '468': 490,\n",
      "    '469': 491,\n",
      "    '47': 69,\n",
      "    '470': 492,\n",
      "    '471': 493,\n",
      "    '472': 494,\n",
      "    '473': 495,\n",
      "    '474': 496,\n",
      "    '475': 497,\n",
      "    '476': 498,\n",
      "    '477': 499,\n",
      "    '478': 500,\n",
      "    '479': 501,\n",
      "    '48': 70,\n",
      "    '480': 502,\n",
      "    '481': 503,\n",
      "    '482': 504,\n",
      "    '483': 505,\n",
      "    '484': 506,\n",
      "    '485': 507,\n",
      "    '486': 508,\n",
      "    '487': 509,\n",
      "    '488': 510,\n",
      "    '489': 511,\n",
      "    '49': 71,\n",
      "    '490': 512,\n",
      "    '491': 513,\n",
      "    '492': 514,\n",
      "    '493': 515,\n",
      "    '494': 516,\n",
      "    '495': 517,\n",
      "    '496': 518,\n",
      "    '497': 519,\n",
      "    '498': 520,\n",
      "    '499': 521,\n",
      "    '5': 27,\n",
      "    '50': 72,\n",
      "    '500': 522,\n",
      "    '501': 523,\n",
      "    '502': 524,\n",
      "    '503': 525,\n",
      "    '504': 526,\n",
      "    '505': 527,\n",
      "    '506': 528,\n",
      "    '507': 529,\n",
      "    '508': 530,\n",
      "    '509': 531,\n",
      "    '51': 73,\n",
      "    '510': 532,\n",
      "    '511': 533,\n",
      "    '52': 74,\n",
      "    '53': 75,\n",
      "    '54': 76,\n",
      "    '55': 77,\n",
      "    '56': 78,\n",
      "    '57': 79,\n",
      "    '58': 80,\n",
      "    '59': 81,\n",
      "    '6': 28,\n",
      "    '60': 82,\n",
      "    '61': 83,\n",
      "    '62': 84,\n",
      "    '63': 85,\n",
      "    '64': 86,\n",
      "    '65': 87,\n",
      "    '66': 88,\n",
      "    '67': 89,\n",
      "    '68': 90,\n",
      "    '69': 91,\n",
      "    '7': 29,\n",
      "    '70': 92,\n",
      "    '71': 93,\n",
      "    '72': 94,\n",
      "    '73': 95,\n",
      "    '74': 96,\n",
      "    '75': 97,\n",
      "    '76': 98,\n",
      "    '77': 99,\n",
      "    '78': 100,\n",
      "    '79': 101,\n",
      "    '8': 30,\n",
      "    '80': 102,\n",
      "    '81': 103,\n",
      "    '82': 104,\n",
      "    '83': 105,\n",
      "    '84': 106,\n",
      "    '85': 107,\n",
      "    '86': 108,\n",
      "    '87': 109,\n",
      "    '88': 110,\n",
      "    '89': 111,\n",
      "    '9': 31,\n",
      "    '90': 112,\n",
      "    '91': 113,\n",
      "    '92': 114,\n",
      "    '93': 115,\n",
      "    '94': 116,\n",
      "    '95': 117,\n",
      "    '96': 118,\n",
      "    '97': 119,\n",
      "    '98': 120,\n",
      "    '99': 121,\n",
      "    '<->': 546,\n",
      "    '<.>': 545,\n",
      "    '<0>': 547,\n",
      "    '<1>': 548,\n",
      "    '<2>': 549,\n",
      "    '<3>': 550,\n",
      "    '<4>': 551,\n",
      "    '<5>': 552,\n",
      "    '<6>': 553,\n",
      "    '<7>': 554,\n",
      "    '<8>': 555,\n",
      "    '<9>': 556,\n",
      "    '<bos>': 20,\n",
      "    '<e>': 544,\n",
      "    '<edge_bi>': 17,\n",
      "    '<edge_in>': 15,\n",
      "    '<edge_jump>': 18,\n",
      "    '<edge_out>': 16,\n",
      "    '<eos>': 19,\n",
      "    '<gsum>': 14,\n",
      "    '<icl>': 2,\n",
      "    '<label_pad>': -100,\n",
      "    '<mask>': 1,\n",
      "    '<new>': 21,\n",
      "    '<sep>': 3,\n",
      "    'molecule#edge#0': 740,\n",
      "    'molecule#edge#0#0': 743,\n",
      "    'molecule#edge#0#1': 744,\n",
      "    'molecule#edge#0#2': 745,\n",
      "    'molecule#edge#0#3': 746,\n",
      "    'molecule#edge#0#4': 747,\n",
      "    'molecule#edge#1': 741,\n",
      "    'molecule#edge#1#0': 748,\n",
      "    'molecule#edge#1#1': 749,\n",
      "    'molecule#edge#1#2': 750,\n",
      "    'molecule#edge#1#3': 751,\n",
      "    'molecule#edge#1#4': 752,\n",
      "    'molecule#edge#1#5': 753,\n",
      "    'molecule#edge#2': 742,\n",
      "    'molecule#edge#2#0': 754,\n",
      "    'molecule#edge#2#1': 755,\n",
      "    'molecule#node#0': 557,\n",
      "    'molecule#node#0#0': 566,\n",
      "    'molecule#node#0#1': 567,\n",
      "    'molecule#node#0#10': 568,\n",
      "    'molecule#node#0#100': 569,\n",
      "    'molecule#node#0#101': 570,\n",
      "    'molecule#node#0#102': 571,\n",
      "    'molecule#node#0#103': 572,\n",
      "    'molecule#node#0#104': 573,\n",
      "    'molecule#node#0#105': 574,\n",
      "    'molecule#node#0#106': 575,\n",
      "    'molecule#node#0#107': 576,\n",
      "    'molecule#node#0#108': 577,\n",
      "    'molecule#node#0#109': 578,\n",
      "    'molecule#node#0#11': 579,\n",
      "    'molecule#node#0#110': 580,\n",
      "    'molecule#node#0#111': 581,\n",
      "    'molecule#node#0#112': 582,\n",
      "    'molecule#node#0#113': 583,\n",
      "    'molecule#node#0#114': 584,\n",
      "    'molecule#node#0#115': 585,\n",
      "    'molecule#node#0#116': 586,\n",
      "    'molecule#node#0#117': 587,\n",
      "    'molecule#node#0#118': 588,\n",
      "    'molecule#node#0#12': 589,\n",
      "    'molecule#node#0#13': 590,\n",
      "    'molecule#node#0#14': 591,\n",
      "    'molecule#node#0#15': 592,\n",
      "    'molecule#node#0#16': 593,\n",
      "    'molecule#node#0#17': 594,\n",
      "    'molecule#node#0#18': 595,\n",
      "    'molecule#node#0#19': 596,\n",
      "    'molecule#node#0#2': 597,\n",
      "    'molecule#node#0#20': 598,\n",
      "    'molecule#nod ......\n",
      "label token id to be converted to -100 is {2}\n"
     ]
    }
   ],
   "source": [
    "add_eos = False\n",
    "rank = 0\n",
    "stack_method = \"short\"\n",
    "# 1.3 build vocab and then init tokenizer from the tokenization config\n",
    "vocab_builder.build_vocab(raw_dataset, tokenizer_config, rank) # build vocab from file or scratch\n",
    "tokenizer_cls = getattr(tokenizer, tokenizer_config[\"tokenizer_class\"]) # StackGSTTokenizer, custom defined\n",
    "gtokenizer = tokenizer_cls(\n",
    "    tokenizer_config, add_eos=add_eos, stack_method=stack_method # instantiate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get set and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset is not IterableDataset\n",
      "train_sampler:  3452151\n",
      "first 10 elements in train_sampler:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "first 10 elements in train_sampler:  [1194340, 3418645, 2107718, 2496436, 1786570, 1255597, 1883572, 1780019, 412330, 1569708]\n",
      "False 1194340 3452151\n"
     ]
    }
   ],
   "source": [
    "world_size = 1\n",
    "\n",
    "# 1.4 get train/test sampler\n",
    "train_dataset = dataset  #         idx = dataset.sampler[0] # (0, Data(edge_index=[2, 40], edge_attr=[40, 3], x=[18, 9], y=[1, 1], num_nodes=18, idx=0, idx_of_ds=0))\n",
    "if not isinstance(train_dataset, IterableDataset): # True\n",
    "    print(\"train_dataset is not IterableDataset\")\n",
    "    train_sampler = train_dataset.sampler\n",
    "    print(\"train_sampler: \", len(train_sampler))\n",
    "    print(\"first 10 elements in train_sampler: \", train_sampler[:10])\n",
    "    random.shuffle(train_sampler)\n",
    "    print(\"first 10 elements in train_sampler: \", train_sampler[:10])\n",
    "    train_shuffle, train_sampler, train_cnt = set_up_shuffle_and_sampler( # train_shuffle = False, sampler, len(sampler)\n",
    "        train_dataset, train_sampler\n",
    "    )\n",
    "    print(train_shuffle, train_sampler[0], train_cnt)\n",
    "else: \n",
    "    train_cnt = len(train_dataset) * world_size # why # 1\n",
    "    train_sampler = None\n",
    "    train_shuffle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pack_tokens is 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:36<00:00, 274.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated tokens per sample 20.0 with std 4.0 using 10000 samples and mpe 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pack_tokens = 0\n",
    "max_position_embeddings = 1024\n",
    "\n",
    "if pack_tokens > 0: # 0\n",
    "    gtokenizer.mpe = max_position_embeddings\n",
    "    # cannot pass `iter(train_dataset)` for Iterable ds, because `TypeError: cannot pickle 'generator' object`\n",
    "    gtokenizer.dataset = train_dataset\n",
    "    gtokenizer.sampler = tuple(train_sampler) if train_sampler is not None else None\n",
    "    gtokenizer.random_ratio = pack_tokens\n",
    "    tokens_per_sample = max_position_embeddings\n",
    "else:\n",
    "    print(\"pack_tokens is 0\")\n",
    "    tokens_per_sample = misc_utils.estimate_tokens_per_sample(\n",
    "        gtokenizer,\n",
    "        train_dataset,\n",
    "        train_sampler,\n",
    "        max_position_embeddings,\n",
    "        world_size,\n",
    "    ) # Estimated tokens per sample 20.0 with std 4.0 using 10000 samples and mpe 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2024-12-09 01:28:38.663031] tokens_per_sample: 20.0\n",
      "\n",
      "Inspecting graph of index 1194340\n",
      "Inspecting tokenization results!\n",
      "Tokenize graph:\n",
      "Data(edge_index=[2, 34], edge_attr=[34, 3], x=[17, 9], y=[1, 1], num_nodes=17, idx=1194340, idx_of_ds=0)\n",
      "\n",
      "Tokens:\n",
      "[['295', 'molecule#node#0#5', 'molecule#node#1#2', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0', 'molecule#edge#1', 'molecule#edge#2'],\n",
      " ['296', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#3', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['295', 'molecule#node#0#5', 'molecule#node#1#2', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['297', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#2', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['298', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['299', 'molecule#node#0#7', 'molecule#node#1#0', 'molecule#node#2#2', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['298', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['300', 'molecule#node#0#7', 'molecule#node#1#0', 'molecule#node#2#1', 'molecule#node#3#5', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#1', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['298', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#1', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['297', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#2', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['295', 'molecule#node#0#5', 'molecule#node#1#2', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['301', 'molecule#node#0#6', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['302', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#2', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['303', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#1', 'molecule#node#8#1', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['304', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#1', 'molecule#node#8#1', 'molecule#edge#0#3', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['305', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#3', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['304', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#1', 'molecule#node#8#1', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['306', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#1', 'molecule#node#8#1', 'molecule#edge#0#3', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['307', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#1', 'molecule#node#8#1', 'molecule#edge#0#3', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['308', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#1', 'molecule#node#8#1', 'molecule#edge#0#3', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['309', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#1', 'molecule#node#8#1', 'molecule#edge#0#3', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['303', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#1', 'molecule#node#8#1', 'molecule#edge#0#3', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['302', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#2', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['301', 'molecule#node#0#6', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['310', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#2', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['311', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#3', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0']]\n",
      "Labels:\n",
      "[['296', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#3', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['295', 'molecule#node#0#5', 'molecule#node#1#2', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['297', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#2', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['298', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['299', 'molecule#node#0#7', 'molecule#node#1#0', 'molecule#node#2#2', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['298', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['300', 'molecule#node#0#7', 'molecule#node#1#0', 'molecule#node#2#1', 'molecule#node#3#5', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#1', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['298', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#1', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['297', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#2', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['295', 'molecule#node#0#5', 'molecule#node#1#2', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['301', 'molecule#node#0#6', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['302', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#2', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['303', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#1', 'molecule#node#8#1', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['304', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#1', 'molecule#node#8#1', 'molecule#edge#0#3', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['305', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#3', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['304', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#1', 'molecule#node#8#1', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['306', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#1', 'molecule#node#8#1', 'molecule#edge#0#3', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['307', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#1', 'molecule#node#8#1', 'molecule#edge#0#3', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['308', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#1', 'molecule#node#8#1', 'molecule#edge#0#3', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['309', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#1', 'molecule#node#8#1', 'molecule#edge#0#3', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['303', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#1', 'molecule#node#8#1', 'molecule#edge#0#3', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['302', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#2', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['301', 'molecule#node#0#6', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['310', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#2', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['311', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#3', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']]\n",
      "embed:[]\n",
      "\n",
      "Tokenized results:\n",
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'embed': array([], shape=(26, 0), dtype=float64),\n",
      " 'input_ids': [[317, 630, 687, 696, 709, 715, 724, 732, 736, 738, 740, 741, 742],\n",
      "               [318, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [317, 630, 687, 696, 709, 715, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [319, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [320, 630, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 754],\n",
      "               [321, 652, 685, 694, 709, 715, 724, 731, 736, 738, 743, 748, 755],\n",
      "               [320, 630, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "               [322, 652, 685, 691, 709, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "               [320, 630, 685, 695, 709, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "               [319, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [317, 630, 687, 696, 709, 715, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [323, 641, 685, 695, 709, 714, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [324, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [325, 630, 685, 695, 709, 714, 724, 731, 737, 739, 743, 748, 754],\n",
      "               [326, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "               [327, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [326, 630, 685, 695, 709, 714, 724, 731, 737, 739, 743, 748, 754],\n",
      "               [328, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "               [329, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "               [330, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "               [331, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "               [325, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "               [324, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [323, 641, 685, 695, 709, 714, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [332, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [333, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754]],\n",
      " 'labels': [[318, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [317, 630, 687, 696, 709, 715, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [319, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [320, 630, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 754],\n",
      "            [321, 652, 685, 694, 709, 715, 724, 731, 736, 738, 743, 748, 755],\n",
      "            [320, 630, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "            [322, 652, 685, 691, 709, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "            [320, 630, 685, 695, 709, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "            [319, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [317, 630, 687, 696, 709, 715, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [323, 641, 685, 695, 709, 714, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [324, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [325, 630, 685, 695, 709, 714, 724, 731, 737, 739, 743, 748, 754],\n",
      "            [326, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [327, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [326, 630, 685, 695, 709, 714, 724, 731, 737, 739, 743, 748, 754],\n",
      "            [328, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [329, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [330, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [331, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [325, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [324, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [323, 641, 685, 695, 709, 714, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [332, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [333, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19]],\n",
      " 'position_ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]}\n",
      "\n",
      "Inputs for model:\n",
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'embed': array([], shape=(26, 0), dtype=float64),\n",
      " 'input_ids': [[317, 630, 687, 696, 709, 715, 724, 732, 736, 738, 740, 741, 742],\n",
      "               [318, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [317, 630, 687, 696, 709, 715, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [319, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [320, 630, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 754],\n",
      "               [321, 652, 685, 694, 709, 715, 724, 731, 736, 738, 743, 748, 755],\n",
      "               [320, 630, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "               [322, 652, 685, 691, 709, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "               [320, 630, 685, 695, 709, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "               [319, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [317, 630, 687, 696, 709, 715, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [323, 641, 685, 695, 709, 714, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [324, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [325, 630, 685, 695, 709, 714, 724, 731, 737, 739, 743, 748, 754],\n",
      "               [326, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "               [327, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [326, 630, 685, 695, 709, 714, 724, 731, 737, 739, 743, 748, 754],\n",
      "               [328, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "               [329, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "               [330, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "               [331, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "               [325, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "               [324, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [323, 641, 685, 695, 709, 714, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [332, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [333, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754]],\n",
      " 'labels': [[318, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [317, 630, 687, 696, 709, 715, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [319, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [320, 630, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 754],\n",
      "            [321, 652, 685, 694, 709, 715, 724, 731, 736, 738, 743, 748, 755],\n",
      "            [320, 630, 685, 695, 709, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "            [322, 652, 685, 691, 709, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "            [320, 630, 685, 695, 709, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "            [319, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [317, 630, 687, 696, 709, 715, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [323, 641, 685, 695, 709, 714, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [324, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [325, 630, 685, 695, 709, 714, 724, 731, 737, 739, 743, 748, 754],\n",
      "            [326, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [327, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [326, 630, 685, 695, 709, 714, 724, 731, 737, 739, 743, 748, 754],\n",
      "            [328, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [329, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [330, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [331, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [325, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [324, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [323, 641, 685, 695, 709, 714, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [332, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [333, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19]],\n",
      " 'position_ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]}\n",
      "\n",
      "[Warning] Set eos_idx to 100000000 for task pretrain!\n",
      "gtokenizer.dataset:  None\n",
      "\n",
      "[2024-12-09 01:28:38.678786] total_num_steps: 48829\n",
      "warmup_num_steps: 4883\n",
      "epochs per worker: 15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task_type='pretrain'\n",
    "total_tokens=1e9\n",
    "batch_size = 1024\n",
    "warmup_tokens=1e8\n",
    "\n",
    "tokens_per_sample = (\n",
    "    tokens_per_sample // 2 if task_type == \"pretrain-euler\" else tokens_per_sample\n",
    ")\n",
    "print(f\"\\n[{datetime.now()}] tokens_per_sample: {tokens_per_sample}\") # 20 what is this: estimated tokens per sample, by 10000 samples and mpe 1024\n",
    "\n",
    "inspect_tokenization_results(dataset, gtokenizer) # print out tokenization results, one sample\n",
    "# re-initialize `gtokenizer.dataset` to avoid `TypeError: cannot pickle 'generator' object`\n",
    "gtokenizer.dataset = train_dataset if pack_tokens > 0 else None\n",
    "print(\"gtokenizer.dataset: \", gtokenizer.dataset)\n",
    "\n",
    "total_num_steps = int(\n",
    "    math.ceil(total_tokens / (tokens_per_sample * batch_size * world_size)) # total_tokens defined in config 4e9/(20*1024*1) = 195313\n",
    ")\n",
    "warmup_num_steps = int(\n",
    "    math.ceil(warmup_tokens / (tokens_per_sample * batch_size * world_size)) # 1e8 ...\n",
    ")\n",
    "tmp_cnt = len(train_sampler) if train_sampler else train_cnt / world_size # train_cnt = len(train_dataset) * world_size\n",
    "epochs = int(math.ceil(total_tokens / (tmp_cnt * tokens_per_sample * world_size))) # token for training / token in the dataset = epochs\n",
    "print(\n",
    "    f\"\\n[{datetime.now()}] total_num_steps: {total_num_steps}\\nwarmup_num_steps: {warmup_num_steps}\\nepochs per worker: {epochs}\\n\" # 61 epochs\n",
    ")\n",
    "# 195313 4883 61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphGPTConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 20,\n",
      "  \"causal_attention\": true,\n",
      "  \"cls_token_id\": null,\n",
      "  \"dropout\": 0,\n",
      "  \"embed_dim\": 0,\n",
      "  \"embed_pdrop\": 0,\n",
      "  \"eos_token_id\": 19,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_scale_init_value\": 0,\n",
      "  \"loss_type\": null,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"mlp\": [],\n",
      "  \"mlp_pdrop\": 0,\n",
      "  \"model_type\": \"graphgpt\",\n",
      "  \"next_n_token\": 13,\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 8,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_neg\": null,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"path_pdrop\": 0,\n",
      "  \"pooling_method\": \"last\",\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"stack_method\": \"short\",\n",
      "  \"stacked_feat\": 13,\n",
      "  \"stacked_feat_agg_method\": \"gated\",\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 756\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"./zhang_test/model_config.pkl\", \"rb\") as file:  # \"rb\" mode for reading binary\n",
    "    config = pickle.load(file)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_deepspeed = True\n",
    "\n",
    "# # 2.2 create model\n",
    "# if use_deepspeed:\n",
    "#     deepspeed.init_distributed(\n",
    "#         dist_backend=\"nccl\", rank=rank, world_size=world_size\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphModel(config)\n",
    "\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "# silence the warnings. Please re-enable for inference!\n",
    "model.config.use_cache = False\n",
    "print_trainable_parameters(model) # 235368960"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
