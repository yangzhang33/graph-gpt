{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import fire\n",
    "import copy\n",
    "import multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.cuda.amp import GradScaler\n",
    "# import deepspeed\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "from pprint import pprint, pformat\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from timm.utils import ModelEmaV3\n",
    "from timm.models import load_checkpoint \n",
    "from timm.utils.model import unwrap_model, get_state_dict\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ModuleNotFoundError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "import sys\n",
    "\n",
    "# sys.path.insert(0, \"..\")\n",
    "sys.path.insert(0, \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-16 17:22:29,478] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/miniconda3/envs/graph_gpt/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/yang/miniconda3/envs/graph_gpt/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlvsym'\n",
      "/home/yang/miniconda3/envs/graph_gpt/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/home/yang/miniconda3/envs/graph_gpt/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/home/yang/miniconda3/envs/graph_gpt/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/home/yang/miniconda3/envs/graph_gpt/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "/home/yang/miniconda3/envs/graph_gpt/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `shm_open'\n",
      "/home/yang/miniconda3/envs/graph_gpt/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `shm_unlink'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "from src.data import (\n",
    "    collator,\n",
    "    vocab_builder,\n",
    "    tokenizer,\n",
    "    read_dataset,\n",
    "    OdpsTableIterableDataset,\n",
    ")\n",
    "from src.models import (\n",
    "    GraphGPTConfig,\n",
    "    GraphGPTCausal,\n",
    "    GraphGPT2Config,\n",
    "    GraphGPT2Causal,\n",
    "    GraphBertConfig,\n",
    "    GraphBertForMaskedLM,\n",
    ")\n",
    "from src.utils import (\n",
    "    conf_utils,\n",
    "    loss_utils,\n",
    "    loader_utils,\n",
    "    tokenizer_utils,\n",
    "    modules_utils,\n",
    "    misc_utils,\n",
    "    print_trainable_parameters,\n",
    "    print_params,\n",
    "    inspect_tokenization_results,\n",
    "    set_up_shuffle_and_sampler,\n",
    "    worker_init_fn_seed,\n",
    ")\n",
    "\n",
    "dict_models = {\n",
    "    \"graphgpt2\": (GraphGPT2Causal, GraphGPT2Config),\n",
    "    \"graphgpt\": (GraphGPTCausal, GraphGPTConfig),\n",
    "    \"graphbert\": (GraphBertForMaskedLM, GraphBertConfig),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir: str = \"../data/OGB\"\n",
    "tables: str = \"\"\n",
    "# deepspeed_config = \"./examples/ds_config2_pt.json\"\n",
    "intermediate_size = 0\n",
    "num_attention_heads = 0\n",
    "hidden_size = 512\n",
    "num_hidden_layers = 8\n",
    "task_type='pretrain'\n",
    "causal_attention = 1\n",
    "lr=3e-4\n",
    "model_type = 'graphgpt'\n",
    "output_dir='./exp/models/pcqm4m-v2/test'\n",
    "pretrain_cpt = '/datalake/datastore1/yang/graph-gpt/exp/models/pcqm4m-v2/medium_ntp/pt_ns_h512_l8_b8192_mpe1024_tk1e9_gelu_pretrain3.3m_nmlm_mrlinear_mtp0.8_0_0.2_lr3e-4_adp0.1_pdp0_edp0_mdp0_lsi0_short_gated_wd0.1'\n",
    "samples_per_saving=1000000\n",
    "\n",
    "batch_size = 1024\n",
    "stack_method = 'short'\n",
    "\n",
    "pack_tokens = 0\n",
    "max_position_embeddings = 1024\n",
    "\n",
    "task_type='pretrain'\n",
    "total_tokens=1e9\n",
    "batch_size = 1024\n",
    "warmup_tokens=1e8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_size: 512 intermediate_size: 2048 num_attention_heads: 8 num_hidden_layers: 8 causal_attention: 1\n",
      "gpu_name: NVIDIA RTX A6000 GraphModel: <class 'src.models.graphgpt.modeling_graphgpt.GraphGPTCausal'> GraphModelConfig: <class 'src.models.graphgpt.configuration_graphgpt.GraphGPTConfig'>\n"
     ]
    }
   ],
   "source": [
    "use_tb_writer = False           # use tensorboard writer\n",
    "use_ema = False # False # use exponential moving average to smooth model\n",
    "use_deepspeed = False # True # use deepspeed for training, good to set scheduler\n",
    "if (intermediate_size == 0) and (num_attention_heads == 0): # True\n",
    "    (\n",
    "        hidden_size,\n",
    "        intermediate_size,\n",
    "        num_attention_heads,\n",
    "        num_hidden_layers,\n",
    "    ) = modules_utils.set_up_model_architect(\n",
    "        hidden_size=hidden_size, num_hidden_layers=num_hidden_layers # 768 24 related to model names intermediate_size = hidden_size * 4, num_attention_heads = hidden_size // 64\n",
    "    )# 768 3072 12 24\n",
    "causal_attention = 0 if task_type == \"pretrain-mlm\" else causal_attention\n",
    "print('hidden_size:', hidden_size, 'intermediate_size:', intermediate_size, 'num_attention_heads:', num_attention_heads, 'num_hidden_layers:', num_hidden_layers, 'causal_attention:', causal_attention) # 768 3072 12 24 1\n",
    "\n",
    "\n",
    "# #########################\n",
    "# betas = (0.9, 0.95) # used in AdamW optimizer, important for config beta\n",
    "# #########################\n",
    "# # lr * 0.1 -> from llama2 pre-train settings\n",
    "# min_lr = lr * 0.1 if use_deepspeed else 0    # used in scheduler, when not using deepspeed.\n",
    "# #########################\n",
    "gpu_name = torch.cuda.get_device_name()\n",
    "GraphModel, GraphModelConfig = dict_models[model_type] # Not instantiate yet\n",
    "print('gpu_name:', gpu_name, 'GraphModel:', GraphModel, 'GraphModelConfig:', GraphModelConfig) \n",
    "\n",
    "if os.path.exists(os.path.join(output_dir, \"log.csv\")):\n",
    "    print(\n",
    "        f\"log file {os.path.join(output_dir, 'log.csv')} exists, resume training from {output_dir} instead of initializing from pre-train ckp {pretrain_cpt}!\"\n",
    "    )\n",
    "    pretrain_cpt = output_dir\n",
    "\n",
    "\n",
    "# # 0. init distributed train and get gpu/device info\n",
    "# dist.init_process_group(backend=\"nccl\", init_method=\"env://\")  # for distributed training\n",
    "# dist.barrier() # for sync training\n",
    "# world_size = dist.get_world_size() # 1 # number of GPUs\n",
    "# rank = dist.get_rank() # 0 # current GPU index\n",
    "# local_rank = os.environ.get(\"LOCAL_RANK\") # 0 # current GPU index local to the node\n",
    "# print(f\"\\nworld size: {world_size}, rank: {rank}, local rank: {local_rank}\") # 1 0 0\n",
    "# rnd_seed = torch.random.initial_seed() - rank\n",
    "# random.seed(rnd_seed)\n",
    "# print(f\"seed random with {rnd_seed}\") # 1234\n",
    "# steps_per_saving = samples_per_saving // (world_size * batch_size) # 1000000 // (1 * 1024) = 976\n",
    "# print(f\"\\nsteps_per_saving: {steps_per_saving}\") # 976\n",
    "# params = print_params(**locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attr_world_identifier': 'molecule',\n",
      " 'data_dir': './data/OGB',\n",
      " 'dataset': 'PCQM4Mv2',\n",
      " 'ensemble_datasets': [],\n",
      " 'label_tokens_to_pad': ['<icl>'],\n",
      " 'name_or_path': './data/OGB/pcqm4m-v2',\n",
      " 'pretrain_mlm': {'info': 'name->polynomial|cosine|fixed,power->3/2/1/0.5',\n",
      "                  'name': 'polynomial',\n",
      "                  'params': {'fixed_ratio': 0.7,\n",
      "                             'mtp': [0.8, 0, 0.2],\n",
      "                             'power': 1}},\n",
      " 'sampling': None,\n",
      " 'semantics': {'attr_assignment': 'first',\n",
      "               'attr_shuffle': False,\n",
      "               'common': {'numbers': ['<e>',\n",
      "                                      '<.>',\n",
      "                                      '<->',\n",
      "                                      '<0>',\n",
      "                                      '<1>',\n",
      "                                      '<2>',\n",
      "                                      '<3>',\n",
      "                                      '<4>',\n",
      "                                      '<5>',\n",
      "                                      '<6>',\n",
      "                                      '<7>',\n",
      "                                      '<8>',\n",
      "                                      '<9>'],\n",
      "                          'reserved_token': ['semantics_0',\n",
      "                                             'semantics_1',\n",
      "                                             'semantics_2',\n",
      "                                             'semantics_3',\n",
      "                                             'semantics_4',\n",
      "                                             'semantics_5',\n",
      "                                             'semantics_6',\n",
      "                                             'semantics_7',\n",
      "                                             'semantics_8',\n",
      "                                             'semantics_9']},\n",
      "               'edge': {'continuous': None,\n",
      "                        'dim': 3,\n",
      "                        'discrete': 'edge_attr',\n",
      "                        'ignored_val': None},\n",
      "               'graph': {'continuous': None,\n",
      "                         'discrete': None,\n",
      "                         'ignored_val': None},\n",
      "               'instructions': {'enable': False,\n",
      "                                'func': [{'mask_ratio': 0,\n",
      "                                          'name': 'homo_lumo',\n",
      "                                          'valid': 1},\n",
      "                                         {'name': 'cepdb_prop_all',\n",
      "                                          'valid': 0}],\n",
      "                                'name': 'molecule'},\n",
      "               'node': {'continuous': None,\n",
      "                        'dim': 9,\n",
      "                        'discrete': 'x',\n",
      "                        'ignored_val': None}},\n",
      " 'structure': {'common': {'icl_token': '<icl>',\n",
      "                          'mask_token': '<mask>',\n",
      "                          'reserved_token': ['structure_0',\n",
      "                                             'structure_1',\n",
      "                                             'structure_2',\n",
      "                                             'structure_3',\n",
      "                                             'structure_4',\n",
      "                                             'structure_5',\n",
      "                                             'structure_6',\n",
      "                                             'structure_7',\n",
      "                                             'structure_8',\n",
      "                                             'structure_9'],\n",
      "                          'sep_token': '<sep>'},\n",
      "               'edge': {'bi_token': '<edge_bi>',\n",
      "                        'in_token': '<edge_in>',\n",
      "                        'jump_token': '<edge_jump>',\n",
      "                        'out_token': '<edge_out>',\n",
      "                        'remove_edge_type_token': True},\n",
      "               'graph': {'summary_token': '<gsum>'},\n",
      "               'node': {'bos_token': '<bos>',\n",
      "                        'cyclic': 1,\n",
      "                        'eos_token': '<eos>',\n",
      "                        'new_node_token': '<new>',\n",
      "                        'node_scope': 512,\n",
      "                        'scope_base': 512},\n",
      "               'nx': {'enable': False,\n",
      "                      'func': [{'name': 'degree', 'valid': 0},\n",
      "                               {'name': 'triangles', 'valid': 0},\n",
      "                               {'name': 'shortest_path', 'valid': 0},\n",
      "                               {'name': 'shortest_path_length', 'valid': 0},\n",
      "                               {'name': 'eulerian_path', 'valid': 1}]}},\n",
      " 'task_type': 'pretrain',\n",
      " 'tokenizer_class': 'StackedGSTTokenizer',\n",
      " 'vocab_file': 'vocab512_stacked'}\n"
     ]
    }
   ],
   "source": [
    "# tokenizer config loading\n",
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "file_path = \"./zhang_test/tokenizer_config.json\"\n",
    "with open(file_path, \"r\") as json_file:\n",
    "    tokenizer_config = json.load(json_file)\n",
    "\n",
    "# Print the loaded data\n",
    "pprint(tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacked_feat: 13, next_n_token: 13, embed_dim: 0\n"
     ]
    }
   ],
   "source": [
    "# 1.1 read configuration\n",
    "assert \"pretrain\" in tokenizer_config[\"task_type\"]\n",
    "assert (\n",
    "    tokenizer_config[\"semantics\"][\"attr_assignment\"]   # first\n",
    "    in tokenizer_utils.ATTR_ASSIGNMENT_TYPES   # ATTR_ASSIGNMENT_TYPES = {\"first\", \"last\", \"random\", \"all\", \"mix\"}\n",
    ")\n",
    "# pprint(tokenizer_config)\n",
    "if tokenizer_config[\"tokenizer_class\"] == \"StackedGSTTokenizer\":\n",
    "    attr_dim = (\n",
    "        tokenizer_config[\"semantics\"][\"edge\"][\"dim\"] # 3\n",
    "        + tokenizer_config[\"semantics\"][\"node\"][\"dim\"] # 9\n",
    "    ) # 12\n",
    "    assert stack_method in (\"short\", \"long\", None), f\"stack_method: {stack_method}\" # short\n",
    "    if tokenizer_config[\"structure\"][\"edge\"][\"remove_edge_type_token\"]: # True\n",
    "        stacked_feat = 1 + attr_dim\n",
    "    else:\n",
    "        stacked_feat = 2 + attr_dim\n",
    "    next_n_token = stacked_feat\n",
    "else:\n",
    "    stacked_feat = 1\n",
    "    next_n_token = 1 # maybe how many pack of tokens to predict\n",
    "embed_dim = tokenizer_config[\"semantics\"][\"node\"].get(\n",
    "    \"embed_dim\", 0\n",
    ") + tokenizer_config[\"semantics\"][\"edge\"].get(\"embed_dim\", 0) # 0\n",
    "print(\n",
    "    f\"stacked_feat: {stacked_feat}, next_n_token: {next_n_token}, embed_dim: {embed_dim}\" # 13 13 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset PCQM4Mv2 ...\n",
      "\n",
      "dataset._data -> Data(edge_index=[2, 109093626], edge_attr=[109093626, 3], x=[52970652, 9], y=[3746620])\n",
      "In pre-train mode, set all valid data's y to nan!\n",
      "Before setting, y has 294469 NANs\n",
      "After setting, y has 368014 NANs\n",
      "Default process group has not been initialized, please make sure to call init_process_group.\n",
      "\n",
      "Raw indices: 3746620, Removed indices: 0, New indices: 3746620\n",
      "\n",
      "Raw indices: 3746620, Removed indices: 294469, New indices: 3452151\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "[Warning] permute_nodes enabled! edge_attr remains the same; edge_index and node-attrs will be affected!\n",
      "\n",
      "[2024-12-16 17:22:40.398866] NOT RESET samples of GraphsMapDataset of 3452151 graphs for epoch None!\n",
      "idx_tuple: None\n",
      "(0, Data(edge_index=[2, 40], edge_attr=[40, 3], x=[18, 9], y=[1, 1], num_nodes=18, idx=0, idx_of_ds=0))\n"
     ]
    }
   ],
   "source": [
    "# 1.2 get graph dataset\n",
    "dataset, raw_dataset = read_dataset(\n",
    "    name=tokenizer_config[\"dataset\"],   # PCQM4Mv2\n",
    "    # for local data file reading\n",
    "    data_dir=data_dir,   # './data/OGB'\n",
    "    sampling_config=tokenizer_config[\"sampling\"],    # None\n",
    "    # for odps data reading\n",
    "    table=tables,   # \"\"\n",
    "    edge_dim=tokenizer_config[\"semantics\"][\"edge\"][\"dim\"],    # 3\n",
    "    node_dim=tokenizer_config[\"semantics\"][\"node\"][\"dim\"],    # 9\n",
    "    mode=\"train\",\n",
    "    # general\n",
    "    pretrain_mode=True,\n",
    "    # return_valid_test=True,\n",
    "    ensemble_datasets=tokenizer_config.get(\"ensemble_datasets\", []),    # []\n",
    ")\n",
    "reset_samples_per_epoch = (   # what is this  # None for PCQM4Mv2\n",
    "    dataset.reset_samples_per_epoch\n",
    "    if hasattr(dataset, \"reset_samples_per_epoch\")\n",
    "    else False\n",
    ")\n",
    "if isinstance(dataset, IterableDataset):\n",
    "    print(next(iter(dataset))) \n",
    "else: # True\n",
    "    idx = dataset.sampler[0] # (0, Data(edge_index=[2, 40], edge_attr=[40, 3], x=[18, 9], y=[1, 1], num_nodes=18, idx=0, idx_of_ds=0))\n",
    "    print(dataset[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<src.data.dataset_map.GraphsMapDataset object at 0x7fc291e26520>\n",
      "length of dataset: 3452151\n",
      "(1, Data(edge_index=[2, 34], edge_attr=[34, 3], x=[17, 9], y=[1, 1], num_nodes=17, idx=1, idx_of_ds=0))\n",
      "(2, Data(edge_index=[2, 32], edge_attr=[32, 3], x=[16, 9], y=[1, 1], num_nodes=16, idx=2, idx_of_ds=0))\n",
      "####################################################################################################\n",
      "example\n",
      "edge_index:  tensor([[ 9, 14, 14,  0,  0, 15, 15, 10, 10,  1,  1, 13, 10,  4,  4,  6,  6,  5,\n",
      "          5, 16, 16,  3,  3,  2,  2,  8,  8, 12,  8, 11,  2,  7,  5,  0],\n",
      "        [14,  9,  0, 14, 15,  0, 10, 15,  1, 10, 13,  1,  4, 10,  6,  4,  5,  6,\n",
      "         16,  5,  3, 16,  2,  3,  8,  2, 12,  8, 11,  8,  7,  2,  0,  5]])\n",
      "edge_attr:  tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [1, 2, 1],\n",
      "        [1, 2, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [1, 0, 1],\n",
      "        [1, 0, 1],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [3, 0, 1],\n",
      "        [3, 0, 1]])\n",
      "x:  tensor([[7, 0, 1, 5, 0, 0, 1, 0, 0],\n",
      "        [5, 0, 4, 5, 3, 0, 2, 0, 0],\n",
      "        [7, 0, 2, 5, 0, 0, 1, 0, 0],\n",
      "        [5, 0, 3, 5, 1, 0, 1, 1, 1],\n",
      "        [5, 0, 3, 5, 0, 0, 1, 0, 0],\n",
      "        [5, 0, 3, 5, 0, 0, 1, 1, 1],\n",
      "        [5, 0, 3, 5, 1, 0, 1, 0, 0],\n",
      "        [7, 0, 2, 5, 0, 0, 1, 0, 0],\n",
      "        [6, 0, 3, 5, 0, 0, 1, 0, 0],\n",
      "        [5, 0, 4, 5, 3, 0, 2, 0, 0],\n",
      "        [5, 0, 4, 5, 3, 0, 2, 0, 0],\n",
      "        [5, 0, 3, 5, 0, 0, 1, 1, 1],\n",
      "        [5, 0, 3, 5, 1, 0, 1, 1, 1],\n",
      "        [5, 0, 3, 5, 0, 0, 1, 1, 1],\n",
      "        [5, 0, 3, 5, 1, 0, 1, 0, 0],\n",
      "        [5, 0, 3, 5, 1, 0, 1, 1, 1],\n",
      "        [5, 0, 4, 5, 3, 0, 2, 0, 0]])\n",
      "y:  tensor([[4.4110]])\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print('length of dataset:', len(dataset))\n",
    "print(dataset[1])\n",
    "print(dataset[2])\n",
    "print(\"#\" * 100)\n",
    "print(\"example\")\n",
    "print(\"edge_index: \", dataset[1][1].edge_index)\n",
    "print(\"edge_attr: \", dataset[1][1].edge_attr)\n",
    "print(\"x: \", dataset[1][1].x)\n",
    "print(\"y: \", dataset[1][1].y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=CCN(C=Cc1ccccc1C)C(C)=O\n"
     ]
    }
   ],
   "source": [
    "example_graph = dataset[2][1]\n",
    "from src.utils.my_utiles import graph2smiles\n",
    "smiles = graph2smiles(example_graph.edge_index, example_graph.edge_attr, example_graph.x)\n",
    "print(smiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-16 17:22:40.493413] Vocab is already built and saved in ./data/OGB/pcqm4m-v2/vocab512_stacked!\n",
      "[2024-12-16 17:22:40.493746] Loading vocab from ./data/OGB/pcqm4m-v2/vocab512_stacked ...\n",
      "[2024-12-16 17:22:40.495952]\n",
      "{   '0': 22,\n",
      "    '1': 23,\n",
      "    '10': 32,\n",
      "    '100': 122,\n",
      "    '101': 123,\n",
      "    '102': 124,\n",
      "    '103': 125,\n",
      "    '104': 126,\n",
      "    '105': 127,\n",
      "    '106': 128,\n",
      "    '107': 129,\n",
      "    '108': 130,\n",
      "    '109': 131,\n",
      "    '11': 33,\n",
      "    '110': 132,\n",
      "    '111': 133,\n",
      "    '112': 134,\n",
      "    '113': 135,\n",
      "    '114': 136,\n",
      "    '115': 137,\n",
      "    '116': 138,\n",
      "    '117': 139,\n",
      "    '118': 140,\n",
      "    '119': 141,\n",
      "    '12': 34,\n",
      "    '120': 142,\n",
      "    '121': 143,\n",
      "    '122': 144,\n",
      "    '123': 145,\n",
      "    '124': 146,\n",
      "    '125': 147,\n",
      "    '126': 148,\n",
      "    '127': 149,\n",
      "    '128': 150,\n",
      "    '129': 151,\n",
      "    '13': 35,\n",
      "    '130': 152,\n",
      "    '131': 153,\n",
      "    '132': 154,\n",
      "    '133': 155,\n",
      "    '134': 156,\n",
      "    '135': 157,\n",
      "    '136': 158,\n",
      "    '137': 159,\n",
      "    '138': 160,\n",
      "    '139': 161,\n",
      "    '14': 36,\n",
      "    '140': 162,\n",
      "    '141': 163,\n",
      "    '142': 164,\n",
      "    '143': 165,\n",
      "    '144': 166,\n",
      "    '145': 167,\n",
      "    '146': 168,\n",
      "    '147': 169,\n",
      "    '148': 170,\n",
      "    '149': 171,\n",
      "    '15': 37,\n",
      "    '150': 172,\n",
      "    '151': 173,\n",
      "    '152': 174,\n",
      "    '153': 175,\n",
      "    '154': 176,\n",
      "    '155': 177,\n",
      "    '156': 178,\n",
      "    '157': 179,\n",
      "    '158': 180,\n",
      "    '159': 181,\n",
      "    '16': 38,\n",
      "    '160': 182,\n",
      "    '161': 183,\n",
      "    '162': 184,\n",
      "    '163': 185,\n",
      "    '164': 186,\n",
      "    '165': 187,\n",
      "    '166': 188,\n",
      "    '167': 189,\n",
      "    '168': 190,\n",
      "    '169': 191,\n",
      "    '17': 39,\n",
      "    '170': 192,\n",
      "    '171': 193,\n",
      "    '172': 194,\n",
      "    '173': 195,\n",
      "    '174': 196,\n",
      "    '175': 197,\n",
      "    '176': 198,\n",
      "    '177': 199,\n",
      "    '178': 200,\n",
      "    '179': 201,\n",
      "    '18': 40,\n",
      "    '180': 202,\n",
      "    '181': 203,\n",
      "    '182': 204,\n",
      "    '183': 205,\n",
      "    '184': 206,\n",
      "    '185': 207,\n",
      "    '186': 208,\n",
      "    '187': 209,\n",
      "    '188': 210,\n",
      "    '189': 211,\n",
      "    '19': 41,\n",
      "    '190': 212,\n",
      "    '191': 213,\n",
      "    '192': 214,\n",
      "    '193': 215,\n",
      "    '194': 216,\n",
      "    '195': 217,\n",
      "    '196': 218,\n",
      "    '197': 219,\n",
      "    '198': 220,\n",
      "    '199': 221,\n",
      "    '2': 24,\n",
      "    '20': 42,\n",
      "    '200': 222,\n",
      "    '201': 223,\n",
      "    '202': 224,\n",
      "    '203': 225,\n",
      "    '204': 226,\n",
      "    '205': 227,\n",
      "    '206': 228,\n",
      "    '207': 229,\n",
      "    '208': 230,\n",
      "    '209': 231,\n",
      "    '21': 43,\n",
      "    '210': 232,\n",
      "    '211': 233,\n",
      "    '212': 234,\n",
      "    '213': 235,\n",
      "    '214': 236,\n",
      "    '215': 237,\n",
      "    '216': 238,\n",
      "    '217': 239,\n",
      "    '218': 240,\n",
      "    '219': 241,\n",
      "    '22': 44,\n",
      "    '220': 242,\n",
      "    '221': 243,\n",
      "    '222': 244,\n",
      "    '223': 245,\n",
      "    '224': 246,\n",
      "    '225': 247,\n",
      "    '226': 248,\n",
      "    '227': 249,\n",
      "    '228': 250,\n",
      "    '229': 251,\n",
      "    '23': 45,\n",
      "    '230': 252,\n",
      "    '231': 253,\n",
      "    '232': 254,\n",
      "    '233': 255,\n",
      "    '234': 256,\n",
      "    '235': 257,\n",
      "    '236': 258,\n",
      "    '237': 259,\n",
      "    '238': 260,\n",
      "    '239': 261,\n",
      "    '24': 46,\n",
      "    '240': 262,\n",
      "    '241': 263,\n",
      "    '242': 264,\n",
      "    '243': 265,\n",
      "    '244': 266,\n",
      "    '245': 267,\n",
      "    '246': 268,\n",
      "    '247': 269,\n",
      "    '248': 270,\n",
      "    '249': 271,\n",
      "    '25': 47,\n",
      "    '250': 272,\n",
      "    '251': 273,\n",
      "    '252': 274,\n",
      "    '253': 275,\n",
      "    '254': 276,\n",
      "    '255': 277,\n",
      "    '256': 278,\n",
      "    '257': 279,\n",
      "    '258': 280,\n",
      "    '259': 281,\n",
      "    '26': 48,\n",
      "    '260': 282,\n",
      "    '261': 283,\n",
      "    '262': 284,\n",
      "    '263': 285,\n",
      "    '264': 286,\n",
      "    '265': 287,\n",
      "    '266': 288,\n",
      "    '267': 289,\n",
      "    '268': 290,\n",
      "    '269': 291,\n",
      "    '27': 49,\n",
      "    '270': 292,\n",
      "    '271': 293,\n",
      "    '272': 294,\n",
      "    '273': 295,\n",
      "    '274': 296,\n",
      "    '275': 297,\n",
      "    '276': 298,\n",
      "    '277': 299,\n",
      "    '278': 300,\n",
      "    '279': 301,\n",
      "    '28': 50,\n",
      "    '280': 302,\n",
      "    '281': 303,\n",
      "    '282': 304,\n",
      "    '283': 305,\n",
      "    '284': 306,\n",
      "    '285': 307,\n",
      "    '286': 308,\n",
      "    '287': 309,\n",
      "    '288': 310,\n",
      "    '289': 311,\n",
      "    '29': 51,\n",
      "    '290': 312,\n",
      "    '291': 313,\n",
      "    '292': 314,\n",
      "    '293': 315,\n",
      "    '294': 316,\n",
      "    '295': 317,\n",
      "    '296': 318,\n",
      "    '297': 319,\n",
      "    '298': 320,\n",
      "    '299': 321,\n",
      "    '3': 25,\n",
      "    '30': 52,\n",
      "    '300': 322,\n",
      "    '301': 323,\n",
      "    '302': 324,\n",
      "    '303': 325,\n",
      "    '304': 326,\n",
      "    '305': 327,\n",
      "    '306': 328,\n",
      "    '307': 329,\n",
      "    '308': 330,\n",
      "    '309': 331,\n",
      "    '31': 53,\n",
      "    '310': 332,\n",
      "    '311': 333,\n",
      "    '312': 334,\n",
      "    '313': 335,\n",
      "    '314': 336,\n",
      "    '315': 337,\n",
      "    '316': 338,\n",
      "    '317': 339,\n",
      "    '318': 340,\n",
      "    '319': 341,\n",
      "    '32': 54,\n",
      "    '320': 342,\n",
      "    '321': 343,\n",
      "    '322': 344,\n",
      "    '323': 345,\n",
      "    '324': 346,\n",
      "    '325': 347,\n",
      "    '326': 348,\n",
      "    '327': 349,\n",
      "    '328': 350,\n",
      "    '329': 351,\n",
      "    '33': 55,\n",
      "    '330': 352,\n",
      "    '331': 353,\n",
      "    '332': 354,\n",
      "    '333': 355,\n",
      "    '334': 356,\n",
      "    '335': 357,\n",
      "    '336': 358,\n",
      "    '337': 359,\n",
      "    '338': 360,\n",
      "    '339': 361,\n",
      "    '34': 56,\n",
      "    '340': 362,\n",
      "    '341': 363,\n",
      "    '342': 364,\n",
      "    '343': 365,\n",
      "    '344': 366,\n",
      "    '345': 367,\n",
      "    '346': 368,\n",
      "    '347': 369,\n",
      "    '348': 370,\n",
      "    '349': 371,\n",
      "    '35': 57,\n",
      "    '350': 372,\n",
      "    '351': 373,\n",
      "    '352': 374,\n",
      "    '353': 375,\n",
      "    '354': 376,\n",
      "    '355': 377,\n",
      "    '356': 378,\n",
      "    '357': 379,\n",
      "    '358': 380,\n",
      "    '359': 381,\n",
      "    '36': 58,\n",
      "    '360': 382,\n",
      "    '361': 383,\n",
      "    '362': 384,\n",
      "    '363': 385,\n",
      "    '364': 386,\n",
      "    '365': 387,\n",
      "    '366': 388,\n",
      "    '367': 389,\n",
      "    '368': 390,\n",
      "    '369': 391,\n",
      "    '37': 59,\n",
      "    '370': 392,\n",
      "    '371': 393,\n",
      "    '372': 394,\n",
      "    '373': 395,\n",
      "    '374': 396,\n",
      "    '375': 397,\n",
      "    '376': 398,\n",
      "    '377': 399,\n",
      "    '378': 400,\n",
      "    '379': 401,\n",
      "    '38': 60,\n",
      "    '380': 402,\n",
      "    '381': 403,\n",
      "    '382': 404,\n",
      "    '383': 405,\n",
      "    '384': 406,\n",
      "    '385': 407,\n",
      "    '386': 408,\n",
      "    '387': 409,\n",
      "    '388': 410,\n",
      "    '389': 411,\n",
      "    '39': 61,\n",
      "    '390': 412,\n",
      "    '391': 413,\n",
      "    '392': 414,\n",
      "    '393': 415,\n",
      "    '394': 416,\n",
      "    '395': 417,\n",
      "    '396': 418,\n",
      "    '397': 419,\n",
      "    '398': 420,\n",
      "    '399': 421,\n",
      "    '4': 26,\n",
      "    '40': 62,\n",
      "    '400': 422,\n",
      "    '401': 423,\n",
      "    '402': 424,\n",
      "    '403': 425,\n",
      "    '404': 426,\n",
      "    '405': 427,\n",
      "    '406': 428,\n",
      "    '407': 429,\n",
      "    '408': 430,\n",
      "    '409': 431,\n",
      "    '41': 63,\n",
      "    '410': 432,\n",
      "    '411': 433,\n",
      "    '412': 434,\n",
      "    '413': 435,\n",
      "    '414': 436,\n",
      "    '415': 437,\n",
      "    '416': 438,\n",
      "    '417': 439,\n",
      "    '418': 440,\n",
      "    '419': 441,\n",
      "    '42': 64,\n",
      "    '420': 442,\n",
      "    '421': 443,\n",
      "    '422': 444,\n",
      "    '423': 445,\n",
      "    '424': 446,\n",
      "    '425': 447,\n",
      "    '426': 448,\n",
      "    '427': 449,\n",
      "    '428': 450,\n",
      "    '429': 451,\n",
      "    '43': 65,\n",
      "    '430': 452,\n",
      "    '431': 453,\n",
      "    '432': 454,\n",
      "    '433': 455,\n",
      "    '434': 456,\n",
      "    '435': 457,\n",
      "    '436': 458,\n",
      "    '437': 459,\n",
      "    '438': 460,\n",
      "    '439': 461,\n",
      "    '44': 66,\n",
      "    '440': 462,\n",
      "    '441': 463,\n",
      "    '442': 464,\n",
      "    '443': 465,\n",
      "    '444': 466,\n",
      "    '445': 467,\n",
      "    '446': 468,\n",
      "    '447': 469,\n",
      "    '448': 470,\n",
      "    '449': 471,\n",
      "    '45': 67,\n",
      "    '450': 472,\n",
      "    '451': 473,\n",
      "    '452': 474,\n",
      "    '453': 475,\n",
      "    '454': 476,\n",
      "    '455': 477,\n",
      "    '456': 478,\n",
      "    '457': 479,\n",
      "    '458': 480,\n",
      "    '459': 481,\n",
      "    '46': 68,\n",
      "    '460': 482,\n",
      "    '461': 483,\n",
      "    '462': 484,\n",
      "    '463': 485,\n",
      "    '464': 486,\n",
      "    '465': 487,\n",
      "    '466': 488,\n",
      "    '467': 489,\n",
      "    '468': 490,\n",
      "    '469': 491,\n",
      "    '47': 69,\n",
      "    '470': 492,\n",
      "    '471': 493,\n",
      "    '472': 494,\n",
      "    '473': 495,\n",
      "    '474': 496,\n",
      "    '475': 497,\n",
      "    '476': 498,\n",
      "    '477': 499,\n",
      "    '478': 500,\n",
      "    '479': 501,\n",
      "    '48': 70,\n",
      "    '480': 502,\n",
      "    '481': 503,\n",
      "    '482': 504,\n",
      "    '483': 505,\n",
      "    '484': 506,\n",
      "    '485': 507,\n",
      "    '486': 508,\n",
      "    '487': 509,\n",
      "    '488': 510,\n",
      "    '489': 511,\n",
      "    '49': 71,\n",
      "    '490': 512,\n",
      "    '491': 513,\n",
      "    '492': 514,\n",
      "    '493': 515,\n",
      "    '494': 516,\n",
      "    '495': 517,\n",
      "    '496': 518,\n",
      "    '497': 519,\n",
      "    '498': 520,\n",
      "    '499': 521,\n",
      "    '5': 27,\n",
      "    '50': 72,\n",
      "    '500': 522,\n",
      "    '501': 523,\n",
      "    '502': 524,\n",
      "    '503': 525,\n",
      "    '504': 526,\n",
      "    '505': 527,\n",
      "    '506': 528,\n",
      "    '507': 529,\n",
      "    '508': 530,\n",
      "    '509': 531,\n",
      "    '51': 73,\n",
      "    '510': 532,\n",
      "    '511': 533,\n",
      "    '52': 74,\n",
      "    '53': 75,\n",
      "    '54': 76,\n",
      "    '55': 77,\n",
      "    '56': 78,\n",
      "    '57': 79,\n",
      "    '58': 80,\n",
      "    '59': 81,\n",
      "    '6': 28,\n",
      "    '60': 82,\n",
      "    '61': 83,\n",
      "    '62': 84,\n",
      "    '63': 85,\n",
      "    '64': 86,\n",
      "    '65': 87,\n",
      "    '66': 88,\n",
      "    '67': 89,\n",
      "    '68': 90,\n",
      "    '69': 91,\n",
      "    '7': 29,\n",
      "    '70': 92,\n",
      "    '71': 93,\n",
      "    '72': 94,\n",
      "    '73': 95,\n",
      "    '74': 96,\n",
      "    '75': 97,\n",
      "    '76': 98,\n",
      "    '77': 99,\n",
      "    '78': 100,\n",
      "    '79': 101,\n",
      "    '8': 30,\n",
      "    '80': 102,\n",
      "    '81': 103,\n",
      "    '82': 104,\n",
      "    '83': 105,\n",
      "    '84': 106,\n",
      "    '85': 107,\n",
      "    '86': 108,\n",
      "    '87': 109,\n",
      "    '88': 110,\n",
      "    '89': 111,\n",
      "    '9': 31,\n",
      "    '90': 112,\n",
      "    '91': 113,\n",
      "    '92': 114,\n",
      "    '93': 115,\n",
      "    '94': 116,\n",
      "    '95': 117,\n",
      "    '96': 118,\n",
      "    '97': 119,\n",
      "    '98': 120,\n",
      "    '99': 121,\n",
      "    '<->': 546,\n",
      "    '<.>': 545,\n",
      "    '<0>': 547,\n",
      "    '<1>': 548,\n",
      "    '<2>': 549,\n",
      "    '<3>': 550,\n",
      "    '<4>': 551,\n",
      "    '<5>': 552,\n",
      "    '<6>': 553,\n",
      "    '<7>': 554,\n",
      "    '<8>': 555,\n",
      "    '<9>': 556,\n",
      "    '<bos>': 20,\n",
      "    '<e>': 544,\n",
      "    '<edge_bi>': 17,\n",
      "    '<edge_in>': 15,\n",
      "    '<edge_jump>': 18,\n",
      "    '<edge_out>': 16,\n",
      "    '<eos>': 19,\n",
      "    '<gsum>': 14,\n",
      "    '<icl>': 2,\n",
      "    '<label_pad>': -100,\n",
      "    '<mask>': 1,\n",
      "    '<new>': 21,\n",
      "    '<sep>': 3,\n",
      "    'molecule#edge#0': 740,\n",
      "    'molecule#edge#0#0': 743,\n",
      "    'molecule#edge#0#1': 744,\n",
      "    'molecule#edge#0#2': 745,\n",
      "    'molecule#edge#0#3': 746,\n",
      "    'molecule#edge#0#4': 747,\n",
      "    'molecule#edge#1': 741,\n",
      "    'molecule#edge#1#0': 748,\n",
      "    'molecule#edge#1#1': 749,\n",
      "    'molecule#edge#1#2': 750,\n",
      "    'molecule#edge#1#3': 751,\n",
      "    'molecule#edge#1#4': 752,\n",
      "    'molecule#edge#1#5': 753,\n",
      "    'molecule#edge#2': 742,\n",
      "    'molecule#edge#2#0': 754,\n",
      "    'molecule#edge#2#1': 755,\n",
      "    'molecule#node#0': 557,\n",
      "    'molecule#node#0#0': 566,\n",
      "    'molecule#node#0#1': 567,\n",
      "    'molecule#node#0#10': 568,\n",
      "    'molecule#node#0#100': 569,\n",
      "    'molecule#node#0#101': 570,\n",
      "    'molecule#node#0#102': 571,\n",
      "    'molecule#node#0#103': 572,\n",
      "    'molecule#node#0#104': 573,\n",
      "    'molecule#node#0#105': 574,\n",
      "    'molecule#node#0#106': 575,\n",
      "    'molecule#node#0#107': 576,\n",
      "    'molecule#node#0#108': 577,\n",
      "    'molecule#node#0#109': 578,\n",
      "    'molecule#node#0#11': 579,\n",
      "    'molecule#node#0#110': 580,\n",
      "    'molecule#node#0#111': 581,\n",
      "    'molecule#node#0#112': 582,\n",
      "    'molecule#node#0#113': 583,\n",
      "    'molecule#node#0#114': 584,\n",
      "    'molecule#node#0#115': 585,\n",
      "    'molecule#node#0#116': 586,\n",
      "    'molecule#node#0#117': 587,\n",
      "    'molecule#node#0#118': 588,\n",
      "    'molecule#node#0#12': 589,\n",
      "    'molecule#node#0#13': 590,\n",
      "    'molecule#node#0#14': 591,\n",
      "    'molecule#node#0#15': 592,\n",
      "    'molecule#node#0#16': 593,\n",
      "    'molecule#node#0#17': 594,\n",
      "    'molecule#node#0#18': 595,\n",
      "    'molecule#node#0#19': 596,\n",
      "    'molecule#node#0#2': 597,\n",
      "    'molecule#node#0#20': 598,\n",
      "    'molecule#nod ......\n",
      "label token id to be converted to -100 is {2}\n"
     ]
    }
   ],
   "source": [
    "add_eos = False\n",
    "rank = 0\n",
    "stack_method = \"short\"\n",
    "# 1.3 build vocab and then init tokenizer from the tokenization config\n",
    "vocab_builder.build_vocab(raw_dataset, tokenizer_config, rank) # build vocab from file or scratch\n",
    "tokenizer_cls = getattr(tokenizer, tokenizer_config[\"tokenizer_class\"]) # StackGSTTokenizer, custom defined\n",
    "gtokenizer = tokenizer_cls(\n",
    "    tokenizer_config, add_eos=add_eos, stack_method=stack_method # instantiate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<src.data.tokenizer.StackedGSTTokenizer object at 0x7fc16e8d0af0>\n"
     ]
    }
   ],
   "source": [
    "print(gtokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get set and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset is not IterableDataset\n",
      "train_sampler:  3452151\n",
      "first 10 elements in train_sampler:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "first 10 elements in train_sampler:  [2275895, 2727090, 2311376, 1667965, 736249, 735614, 2840383, 1053302, 3240060, 3462391]\n",
      "False 2275895 3452151\n"
     ]
    }
   ],
   "source": [
    "# world_size = 1\n",
    "\n",
    "# 1.4 get train/test sampler\n",
    "train_dataset = dataset  #         idx = dataset.sampler[0] # (0, Data(edge_index=[2, 40], edge_attr=[40, 3], x=[18, 9], y=[1, 1], num_nodes=18, idx=0, idx_of_ds=0))\n",
    "if not isinstance(train_dataset, IterableDataset): # True\n",
    "    print(\"train_dataset is not IterableDataset\")\n",
    "    train_sampler = train_dataset.sampler\n",
    "    print(\"train_sampler: \", len(train_sampler))\n",
    "    print(\"first 10 elements in train_sampler: \", train_sampler[:10])\n",
    "    random.shuffle(train_sampler)\n",
    "    print(\"first 10 elements in train_sampler: \", train_sampler[:10])\n",
    "    train_shuffle, train_sampler, train_cnt = set_up_shuffle_and_sampler( # train_shuffle = False, sampler, len(sampler)\n",
    "        train_dataset, train_sampler\n",
    "    )\n",
    "    print(train_shuffle, train_sampler[0], train_cnt)\n",
    "else: \n",
    "    train_cnt = len(train_dataset) * world_size # why # 1\n",
    "    train_sampler = None\n",
    "    train_shuffle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pack_tokens is 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:44<00:00, 225.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated tokens per sample 20.0 with std 4.0 using 10000 samples and mpe 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "world_size = 1\n",
    "if pack_tokens > 0: # 0\n",
    "    gtokenizer.mpe = max_position_embeddings\n",
    "    # cannot pass `iter(train_dataset)` for Iterable ds, because `TypeError: cannot pickle 'generator' object`\n",
    "    gtokenizer.dataset = train_dataset\n",
    "    gtokenizer.sampler = tuple(train_sampler) if train_sampler is not None else None\n",
    "    gtokenizer.random_ratio = pack_tokens\n",
    "    tokens_per_sample = max_position_embeddings\n",
    "else:\n",
    "    print(\"pack_tokens is 0\")\n",
    "    tokens_per_sample = misc_utils.estimate_tokens_per_sample(\n",
    "        gtokenizer,\n",
    "        train_dataset,\n",
    "        train_sampler,\n",
    "        max_position_embeddings,\n",
    "        world_size,\n",
    "    ) # Estimated tokens per sample 20.0 with std 4.0 using 10000 samples and mpe 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2024-12-16 17:23:26.735709] tokens_per_sample: 20.0\n",
      "\n",
      "Inspecting graph of index 2275895\n",
      "Inspecting tokenization results!\n",
      "Tokenize graph:\n",
      "Data(edge_index=[2, 24], edge_attr=[24, 3], x=[12, 9], y=[1, 1], num_nodes=12, idx=2275895, idx_of_ds=0)\n",
      "\n",
      "Tokens:\n",
      "[['186', 'molecule#node#0#7', 'molecule#node#1#0', 'molecule#node#2#1', 'molecule#node#3#5', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0', 'molecule#edge#1', 'molecule#edge#2'],\n",
      " ['187', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#1', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['188', 'molecule#node#0#5', 'molecule#node#1#1', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#1', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['189', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#2', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#1', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['190', 'molecule#node#0#5', 'molecule#node#1#1', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#1', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['191', 'molecule#node#0#6', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#6', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['192', 'molecule#node#0#7', 'molecule#node#1#0', 'molecule#node#2#1', 'molecule#node#3#4', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['191', 'molecule#node#0#6', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#6', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['193', 'molecule#node#0#7', 'molecule#node#1#0', 'molecule#node#2#1', 'molecule#node#3#5', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#1', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['191', 'molecule#node#0#6', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#6', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#1', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['190', 'molecule#node#0#5', 'molecule#node#1#1', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#1', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['194', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#2', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#1', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['195', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#2', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#1', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['196', 'molecule#node#0#5', 'molecule#node#1#1', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#1', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['197', 'molecule#node#0#7', 'molecule#node#1#0', 'molecule#node#2#2', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['196', 'molecule#node#0#5', 'molecule#node#1#1', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#1', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['188', 'molecule#node#0#5', 'molecule#node#1#1', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#1', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0']]\n",
      "Labels:\n",
      "[['187', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#1', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['188', 'molecule#node#0#5', 'molecule#node#1#1', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#1', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['189', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#2', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#1', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['190', 'molecule#node#0#5', 'molecule#node#1#1', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#1', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['191', 'molecule#node#0#6', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#6', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['192', 'molecule#node#0#7', 'molecule#node#1#0', 'molecule#node#2#1', 'molecule#node#3#4', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['191', 'molecule#node#0#6', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#6', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['193', 'molecule#node#0#7', 'molecule#node#1#0', 'molecule#node#2#1', 'molecule#node#3#5', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#1', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['191', 'molecule#node#0#6', 'molecule#node#1#0', 'molecule#node#2#3', 'molecule#node#3#6', 'molecule#node#4#0', 'molecule#node#5#0', 'molecule#node#6#1', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#1', 'molecule#edge#1#0', 'molecule#edge#2#1'],\n",
      " ['190', 'molecule#node#0#5', 'molecule#node#1#1', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#1', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['194', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#2', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#1', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['195', 'molecule#node#0#5', 'molecule#node#1#0', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#2', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#1', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['196', 'molecule#node#0#5', 'molecule#node#1#1', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#1', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['197', 'molecule#node#0#7', 'molecule#node#1#0', 'molecule#node#2#2', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#0', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['196', 'molecule#node#0#5', 'molecule#node#1#1', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#1', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['188', 'molecule#node#0#5', 'molecule#node#1#1', 'molecule#node#2#4', 'molecule#node#3#5', 'molecule#node#4#1', 'molecule#node#5#0', 'molecule#node#6#2', 'molecule#node#7#0', 'molecule#node#8#1', 'molecule#edge#0#0', 'molecule#edge#1#0', 'molecule#edge#2#0'],\n",
      " ['<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']]\n",
      "embed:[]\n",
      "\n",
      "Tokenized results:\n",
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'embed': array([], shape=(17, 0), dtype=float64),\n",
      " 'input_ids': [[208, 652, 685, 691, 709, 714, 724, 731, 736, 738, 740, 741, 742],\n",
      "               [209, 630, 685, 695, 709, 715, 724, 731, 736, 738, 744, 748, 754],\n",
      "               [210, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "               [211, 630, 685, 696, 709, 716, 724, 732, 736, 739, 743, 748, 754],\n",
      "               [212, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "               [213, 641, 685, 695, 710, 714, 724, 731, 736, 738, 743, 748, 754],\n",
      "               [214, 652, 685, 691, 708, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "               [213, 641, 685, 695, 710, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "               [215, 652, 685, 691, 709, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "               [213, 641, 685, 695, 710, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "               [212, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "               [216, 630, 685, 696, 709, 716, 724, 732, 736, 739, 743, 748, 754],\n",
      "               [217, 630, 685, 696, 709, 716, 724, 732, 736, 739, 743, 748, 754],\n",
      "               [218, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "               [219, 652, 685, 694, 709, 715, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [218, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "               [210, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754]],\n",
      " 'labels': [[209, 630, 685, 695, 709, 715, 724, 731, 736, 738, 744, 748, 754],\n",
      "            [210, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "            [211, 630, 685, 696, 709, 716, 724, 732, 736, 739, 743, 748, 754],\n",
      "            [212, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "            [213, 641, 685, 695, 710, 714, 724, 731, 736, 738, 743, 748, 754],\n",
      "            [214, 652, 685, 691, 708, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "            [213, 641, 685, 695, 710, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "            [215, 652, 685, 691, 709, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "            [213, 641, 685, 695, 710, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "            [212, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "            [216, 630, 685, 696, 709, 716, 724, 732, 736, 739, 743, 748, 754],\n",
      "            [217, 630, 685, 696, 709, 716, 724, 732, 736, 739, 743, 748, 754],\n",
      "            [218, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "            [219, 652, 685, 694, 709, 715, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [218, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "            [210, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "            [19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19]],\n",
      " 'position_ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}\n",
      "\n",
      "Inputs for model:\n",
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'embed': array([], shape=(17, 0), dtype=float64),\n",
      " 'input_ids': [[208, 652, 685, 691, 709, 714, 724, 731, 736, 738, 740, 741, 742],\n",
      "               [209, 630, 685, 695, 709, 715, 724, 731, 736, 738, 744, 748, 754],\n",
      "               [210, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "               [211, 630, 685, 696, 709, 716, 724, 732, 736, 739, 743, 748, 754],\n",
      "               [212, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "               [213, 641, 685, 695, 710, 714, 724, 731, 736, 738, 743, 748, 754],\n",
      "               [214, 652, 685, 691, 708, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "               [213, 641, 685, 695, 710, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "               [215, 652, 685, 691, 709, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "               [213, 641, 685, 695, 710, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "               [212, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "               [216, 630, 685, 696, 709, 716, 724, 732, 736, 739, 743, 748, 754],\n",
      "               [217, 630, 685, 696, 709, 716, 724, 732, 736, 739, 743, 748, 754],\n",
      "               [218, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "               [219, 652, 685, 694, 709, 715, 724, 732, 736, 738, 743, 748, 754],\n",
      "               [218, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "               [210, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754]],\n",
      " 'labels': [[209, 630, 685, 695, 709, 715, 724, 731, 736, 738, 744, 748, 754],\n",
      "            [210, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "            [211, 630, 685, 696, 709, 716, 724, 732, 736, 739, 743, 748, 754],\n",
      "            [212, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "            [213, 641, 685, 695, 710, 714, 724, 731, 736, 738, 743, 748, 754],\n",
      "            [214, 652, 685, 691, 708, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "            [213, 641, 685, 695, 710, 714, 724, 731, 736, 738, 743, 748, 755],\n",
      "            [215, 652, 685, 691, 709, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "            [213, 641, 685, 695, 710, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "            [212, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "            [216, 630, 685, 696, 709, 716, 724, 732, 736, 739, 743, 748, 754],\n",
      "            [217, 630, 685, 696, 709, 716, 724, 732, 736, 739, 743, 748, 754],\n",
      "            [218, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "            [219, 652, 685, 694, 709, 715, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [218, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "            [210, 630, 686, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "            [19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19]],\n",
      " 'position_ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}\n",
      "\n",
      "[Warning] Set eos_idx to 100000000 for task pretrain!\n",
      "gtokenizer.dataset:  None\n",
      "\n",
      "[2024-12-16 17:23:26.748443] total_num_steps: 48829\n",
      "warmup_num_steps: 4883\n",
      "epochs per worker: 15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens_per_sample = (\n",
    "    tokens_per_sample // 2 if task_type == \"pretrain-euler\" else tokens_per_sample\n",
    ")\n",
    "print(f\"\\n[{datetime.now()}] tokens_per_sample: {tokens_per_sample}\") # 20 what is this: estimated tokens per sample, by 10000 samples and mpe 1024\n",
    "\n",
    "inspect_tokenization_results(dataset, gtokenizer) # print out tokenization results, one sample\n",
    "# re-initialize `gtokenizer.dataset` to avoid `TypeError: cannot pickle 'generator' object`\n",
    "gtokenizer.dataset = train_dataset if pack_tokens > 0 else None\n",
    "print(\"gtokenizer.dataset: \", gtokenizer.dataset)\n",
    "\n",
    "total_num_steps = int(\n",
    "    math.ceil(total_tokens / (tokens_per_sample * batch_size * world_size)) # total_tokens defined in config 4e9/(20*1024*1) = 195313\n",
    ")\n",
    "warmup_num_steps = int(\n",
    "    math.ceil(warmup_tokens / (tokens_per_sample * batch_size * world_size)) # 1e8 ...\n",
    ")\n",
    "tmp_cnt = len(train_sampler) if train_sampler else train_cnt / world_size # train_cnt = len(train_dataset) * world_size\n",
    "epochs = int(math.ceil(total_tokens / (tmp_cnt * tokens_per_sample * world_size))) # token for training / token in the dataset = epochs\n",
    "print(\n",
    "    f\"\\n[{datetime.now()}] total_num_steps: {total_num_steps}\\nwarmup_num_steps: {warmup_num_steps}\\nepochs per worker: {epochs}\\n\" # 61 epochs\n",
    ")\n",
    "# 195313 4883 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting tokenization results!\n",
      "Tokenize graph:\n",
      "Data(edge_index=[2, 40], edge_attr=[40, 3], x=[18, 9], y=[1, 1], num_nodes=18, idx=0, idx_of_ds=0)\n",
      "\n",
      "Tokens:\n",
      "[['140',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0',\n",
      "  'molecule#edge#1',\n",
      "  'molecule#edge#2'],\n",
      " ['141',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['142',\n",
      "  'molecule#node#0#6',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#2',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['143',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['144',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['145',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['146',\n",
      "  'molecule#node#0#6',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#2',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#1',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['147',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['148',\n",
      "  'molecule#node#0#7',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#1',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#1',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['147',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#1',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['149',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#2',\n",
      "  'molecule#node#2#4',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#2',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['150',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['151',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['152',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['153',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['154',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#4',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#3',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#2',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['153',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['155',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['156',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['150',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['149',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#2',\n",
      "  'molecule#node#2#4',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#2',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['157',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#1',\n",
      "  'molecule#node#6#2',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['140',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['145',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1']]\n",
      "Labels:\n",
      "[['141',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['142',\n",
      "  'molecule#node#0#6',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#2',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['143',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['144',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['145',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['146',\n",
      "  'molecule#node#0#6',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#2',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#1',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['147',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['148',\n",
      "  'molecule#node#0#7',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#1',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#1',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['147',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#1',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['149',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#2',\n",
      "  'molecule#node#2#4',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#2',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['150',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['151',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['152',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['153',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['154',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#4',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#3',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#2',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#0',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['153',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['155',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['156',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['150',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['149',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#2',\n",
      "  'molecule#node#2#4',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#2',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['157',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#1',\n",
      "  'molecule#node#5#1',\n",
      "  'molecule#node#6#2',\n",
      "  'molecule#node#7#0',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['140',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#0',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#0'],\n",
      " ['145',\n",
      "  'molecule#node#0#5',\n",
      "  'molecule#node#1#0',\n",
      "  'molecule#node#2#3',\n",
      "  'molecule#node#3#5',\n",
      "  'molecule#node#4#0',\n",
      "  'molecule#node#5#0',\n",
      "  'molecule#node#6#1',\n",
      "  'molecule#node#7#1',\n",
      "  'molecule#node#8#1',\n",
      "  'molecule#edge#0#3',\n",
      "  'molecule#edge#1#0',\n",
      "  'molecule#edge#2#1'],\n",
      " ['<eos>',\n",
      "  '<eos>',\n",
      "  '<eos>',\n",
      "  '<eos>',\n",
      "  '<eos>',\n",
      "  '<eos>',\n",
      "  '<eos>',\n",
      "  '<eos>',\n",
      "  '<eos>',\n",
      "  '<eos>',\n",
      "  '<eos>',\n",
      "  '<eos>',\n",
      "  '<eos>']]\n",
      "embed:[]\n",
      "\n",
      "if gtokenizer.mpe is not None,  None\n",
      "Tokenized results:\n",
      "{'attention_mask': [1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1],\n",
      " 'embed': array([], shape=(24, 0), dtype=float64),\n",
      " 'input_ids': [[162,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                740,\n",
      "                741,\n",
      "                742],\n",
      "               [163,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                715,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755],\n",
      "               [164,\n",
      "                641,\n",
      "                685,\n",
      "                694,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755],\n",
      "               [165,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                715,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755],\n",
      "               [166,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                715,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755],\n",
      "               [167,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755],\n",
      "               [168,\n",
      "                641,\n",
      "                685,\n",
      "                694,\n",
      "                709,\n",
      "                714,\n",
      "                725,\n",
      "                731,\n",
      "                736,\n",
      "                739,\n",
      "                743,\n",
      "                748,\n",
      "                755],\n",
      "               [169,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                736,\n",
      "                739,\n",
      "                743,\n",
      "                748,\n",
      "                755],\n",
      "               [170,\n",
      "                652,\n",
      "                685,\n",
      "                691,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                736,\n",
      "                738,\n",
      "                744,\n",
      "                748,\n",
      "                755],\n",
      "               [169,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                736,\n",
      "                739,\n",
      "                744,\n",
      "                748,\n",
      "                755],\n",
      "               [171,\n",
      "                630,\n",
      "                687,\n",
      "                696,\n",
      "                709,\n",
      "                715,\n",
      "                724,\n",
      "                732,\n",
      "                736,\n",
      "                739,\n",
      "                743,\n",
      "                748,\n",
      "                754],\n",
      "               [172,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                743,\n",
      "                748,\n",
      "                754],\n",
      "               [173,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                715,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755],\n",
      "               [174,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                715,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755],\n",
      "               [175,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755],\n",
      "               [176,\n",
      "                630,\n",
      "                685,\n",
      "                696,\n",
      "                709,\n",
      "                717,\n",
      "                724,\n",
      "                732,\n",
      "                736,\n",
      "                738,\n",
      "                743,\n",
      "                748,\n",
      "                754],\n",
      "               [175,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                743,\n",
      "                748,\n",
      "                754],\n",
      "               [177,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                715,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755],\n",
      "               [178,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                715,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755],\n",
      "               [172,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755],\n",
      "               [171,\n",
      "                630,\n",
      "                687,\n",
      "                696,\n",
      "                709,\n",
      "                715,\n",
      "                724,\n",
      "                732,\n",
      "                736,\n",
      "                739,\n",
      "                743,\n",
      "                748,\n",
      "                754],\n",
      "               [179,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                715,\n",
      "                725,\n",
      "                732,\n",
      "                736,\n",
      "                739,\n",
      "                743,\n",
      "                748,\n",
      "                754],\n",
      "               [162,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                743,\n",
      "                748,\n",
      "                754],\n",
      "               [167,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755]],\n",
      " 'labels': [[163, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [164, 641, 685, 694, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [165, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [166, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [167, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [168, 641, 685, 694, 709, 714, 725, 731, 736, 739, 743, 748, 755],\n",
      "            [169, 630, 685, 695, 709, 714, 724, 731, 736, 739, 743, 748, 755],\n",
      "            [170, 652, 685, 691, 709, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "            [169, 630, 685, 695, 709, 714, 724, 731, 736, 739, 744, 748, 755],\n",
      "            [171, 630, 687, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "            [172, 630, 685, 695, 709, 714, 724, 731, 737, 739, 743, 748, 754],\n",
      "            [173, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [174, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [175, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [176, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [175, 630, 685, 695, 709, 714, 724, 731, 737, 739, 743, 748, 754],\n",
      "            [177, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [178, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [172, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [171, 630, 687, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "            [179, 630, 685, 695, 709, 715, 725, 732, 736, 739, 743, 748, 754],\n",
      "            [162, 630, 685, 695, 709, 714, 724, 731, 737, 739, 743, 748, 754],\n",
      "            [167, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19]],\n",
      " 'position_ids': [0,\n",
      "                  1,\n",
      "                  2,\n",
      "                  3,\n",
      "                  4,\n",
      "                  5,\n",
      "                  6,\n",
      "                  7,\n",
      "                  8,\n",
      "                  9,\n",
      "                  10,\n",
      "                  11,\n",
      "                  12,\n",
      "                  13,\n",
      "                  14,\n",
      "                  15,\n",
      "                  16,\n",
      "                  17,\n",
      "                  18,\n",
      "                  19,\n",
      "                  20,\n",
      "                  21,\n",
      "                  22,\n",
      "                  23]}\n",
      "\n",
      "Prepared inputs:\n",
      "{'attention_mask': [1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1,\n",
      "                    1],\n",
      " 'embed': [[],\n",
      "           [],\n",
      "           [],\n",
      "           [],\n",
      "           [],\n",
      "           [],\n",
      "           [],\n",
      "           [],\n",
      "           [],\n",
      "           [],\n",
      "           [],\n",
      "           [],\n",
      "           [],\n",
      "           [],\n",
      "           [],\n",
      "           [],\n",
      "           [],\n",
      "           [],\n",
      "           [],\n",
      "           [],\n",
      "           [],\n",
      "           [],\n",
      "           [],\n",
      "           []],\n",
      " 'input_ids': [[162,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                740,\n",
      "                741,\n",
      "                742],\n",
      "               [163,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                715,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755],\n",
      "               [164,\n",
      "                641,\n",
      "                685,\n",
      "                694,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755],\n",
      "               [165,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                715,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755],\n",
      "               [166,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                715,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755],\n",
      "               [167,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755],\n",
      "               [168,\n",
      "                641,\n",
      "                685,\n",
      "                694,\n",
      "                709,\n",
      "                714,\n",
      "                725,\n",
      "                731,\n",
      "                736,\n",
      "                739,\n",
      "                743,\n",
      "                748,\n",
      "                755],\n",
      "               [169,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                736,\n",
      "                739,\n",
      "                743,\n",
      "                748,\n",
      "                755],\n",
      "               [170,\n",
      "                652,\n",
      "                685,\n",
      "                691,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                736,\n",
      "                738,\n",
      "                744,\n",
      "                748,\n",
      "                755],\n",
      "               [169,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                736,\n",
      "                739,\n",
      "                744,\n",
      "                748,\n",
      "                755],\n",
      "               [171,\n",
      "                630,\n",
      "                687,\n",
      "                696,\n",
      "                709,\n",
      "                715,\n",
      "                724,\n",
      "                732,\n",
      "                736,\n",
      "                739,\n",
      "                743,\n",
      "                748,\n",
      "                754],\n",
      "               [172,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                743,\n",
      "                748,\n",
      "                754],\n",
      "               [173,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                715,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755],\n",
      "               [174,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                715,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755],\n",
      "               [175,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755],\n",
      "               [176,\n",
      "                630,\n",
      "                685,\n",
      "                696,\n",
      "                709,\n",
      "                717,\n",
      "                724,\n",
      "                732,\n",
      "                736,\n",
      "                738,\n",
      "                743,\n",
      "                748,\n",
      "                754],\n",
      "               [175,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                743,\n",
      "                748,\n",
      "                754],\n",
      "               [177,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                715,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755],\n",
      "               [178,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                715,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755],\n",
      "               [172,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755],\n",
      "               [171,\n",
      "                630,\n",
      "                687,\n",
      "                696,\n",
      "                709,\n",
      "                715,\n",
      "                724,\n",
      "                732,\n",
      "                736,\n",
      "                739,\n",
      "                743,\n",
      "                748,\n",
      "                754],\n",
      "               [179,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                715,\n",
      "                725,\n",
      "                732,\n",
      "                736,\n",
      "                739,\n",
      "                743,\n",
      "                748,\n",
      "                754],\n",
      "               [162,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                743,\n",
      "                748,\n",
      "                754],\n",
      "               [167,\n",
      "                630,\n",
      "                685,\n",
      "                695,\n",
      "                709,\n",
      "                714,\n",
      "                724,\n",
      "                731,\n",
      "                737,\n",
      "                739,\n",
      "                746,\n",
      "                748,\n",
      "                755]],\n",
      " 'labels': [[163, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [164, 641, 685, 694, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [165, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [166, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [167, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [168, 641, 685, 694, 709, 714, 725, 731, 736, 739, 743, 748, 755],\n",
      "            [169, 630, 685, 695, 709, 714, 724, 731, 736, 739, 743, 748, 755],\n",
      "            [170, 652, 685, 691, 709, 714, 724, 731, 736, 738, 744, 748, 755],\n",
      "            [169, 630, 685, 695, 709, 714, 724, 731, 736, 739, 744, 748, 755],\n",
      "            [171, 630, 687, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "            [172, 630, 685, 695, 709, 714, 724, 731, 737, 739, 743, 748, 754],\n",
      "            [173, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [174, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [175, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [176, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754],\n",
      "            [175, 630, 685, 695, 709, 714, 724, 731, 737, 739, 743, 748, 754],\n",
      "            [177, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [178, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [172, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [171, 630, 687, 696, 709, 715, 724, 732, 736, 739, 743, 748, 754],\n",
      "            [179, 630, 685, 695, 709, 715, 725, 732, 736, 739, 743, 748, 754],\n",
      "            [162, 630, 685, 695, 709, 714, 724, 731, 737, 739, 743, 748, 754],\n",
      "            [167, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "            [19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19]],\n",
      " 'position_ids': [0,\n",
      "                  1,\n",
      "                  2,\n",
      "                  3,\n",
      "                  4,\n",
      "                  5,\n",
      "                  6,\n",
      "                  7,\n",
      "                  8,\n",
      "                  9,\n",
      "                  10,\n",
      "                  11,\n",
      "                  12,\n",
      "                  13,\n",
      "                  14,\n",
      "                  15,\n",
      "                  16,\n",
      "                  17,\n",
      "                  18,\n",
      "                  19,\n",
      "                  20,\n",
      "                  21,\n",
      "                  22,\n",
      "                  23]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "idx = 0\n",
    "idx2, data = dataset[idx]\n",
    "graph = data\n",
    "print(f\"Inspecting tokenization results!\\nTokenize graph:\\n{data}\")\n",
    "token_res = gtokenizer.tokenize(graph)\n",
    "print(\n",
    "    f\"\\nTokens:\\n{pformat(token_res.ls_tokens)}\\nLabels:\\n{pformat(token_res.ls_labels)}\\nembed:{np.array(token_res.ls_embed)}\\n\"\n",
    ")\n",
    "print(\"if gtokenizer.mpe is not None, \", gtokenizer.mpe)    # None\n",
    "tokens, labels, ls_embed, ls_len = (\n",
    "        gtokenizer.pack_token_seq(token_res, idx)\n",
    "        if gtokenizer.mpe is not None\n",
    "        else (\n",
    "            token_res.ls_tokens,\n",
    "            token_res.ls_labels,\n",
    "            token_res.ls_embed,\n",
    "            [len(token_res.ls_tokens)],\n",
    "        )\n",
    "    )\n",
    "\n",
    "in_dict = gtokenizer.convert_tokens_to_ids(tokens, labels)\n",
    "if ls_embed:  # for pretty print purpose ONLY\n",
    "    in_dict[\"embed\"] = np.array(ls_embed)\n",
    "print(f\"Tokenized results:\\n{pformat(in_dict)}\\n\")\n",
    "if ls_embed:\n",
    "    in_dict[\"embed\"] = ls_embed\n",
    "token_res.ls_tokens = tokens\n",
    "token_res.ls_labels = labels\n",
    "token_res.ls_embed = ls_embed\n",
    "token_res.ls_len = ls_len\n",
    "inputs = gtokenizer.prepare_inputs_for_task(\n",
    "    in_dict,\n",
    "    graph,\n",
    "    token_res=token_res,\n",
    ")\n",
    "print(f\"Prepared inputs:\\n{pformat(inputs)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphGPTConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 20,\n",
      "  \"causal_attention\": true,\n",
      "  \"cls_token_id\": null,\n",
      "  \"dropout\": 0,\n",
      "  \"embed_dim\": 0,\n",
      "  \"embed_pdrop\": 0,\n",
      "  \"eos_token_id\": 19,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_scale_init_value\": 0,\n",
      "  \"loss_type\": null,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"mlp\": [],\n",
      "  \"mlp_pdrop\": 0,\n",
      "  \"model_type\": \"graphgpt\",\n",
      "  \"next_n_token\": 13,\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 8,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_neg\": null,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"path_pdrop\": 0,\n",
      "  \"pooling_method\": \"last\",\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"stack_method\": \"short\",\n",
      "  \"stacked_feat\": 13,\n",
      "  \"stacked_feat_agg_method\": \"gated\",\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 756\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"./zhang_test/model_config.pkl\", \"rb\") as file:  # \"rb\" mode for reading binary\n",
    "    config = pickle.load(file)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_deepspeed = True\n",
    "\n",
    "# # 2.2 create model\n",
    "# if use_deepspeed:\n",
    "#     deepspeed.init_distributed(\n",
    "#         dist_backend=\"nccl\", rank=rank, world_size=world_size\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT Applying dropout in backbone transformer\n",
      "Next-token-prediction changed to next/masked-13-tokens-prediction!\n",
      "trainable params: 37751808 || all params: 37751808 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "model = GraphModel(config)\n",
    "\n",
    "\n",
    "# model.gradient_checkpointing_enable()\n",
    "# silence the warnings. Please re-enable for inference!\n",
    "model.config.use_cache = False\n",
    "print_trainable_parameters(model) # 235368960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained weights from ckp /datalake/datastore1/yang/graph-gpt/exp/models/pcqm4m-v2/medium_ntp/pt_ns_h512_l8_b8192_mpe1024_tk1e9_gelu_pretrain3.3m_nmlm_mrlinear_mtp0.8_0_0.2_lr3e-4_adp0.1_pdp0_edp0_mdp0_lsi0_short_gated_wd0.1/epoch_51\n",
      "inar:  [Errno 2] No such file or directory: '/datalake/datastore1/yang/graph-gpt/exp/models/pcqm4m-v2/medium_ntp/pt_ns_h512_l8_b8192_mpe1024_tk1e9_gelu_pretrain3.3m_nmlm_mrlinear_mtp0.8_0_0.2_lr3e-4_adp0.1_pdp0_edp0_mdp0_lsi0_short_gated_wd0.1/epoch_51/model.pt'\n",
      "Processing zero checkpoint '/datalake/datastore1/yang/graph-gpt/exp/models/pcqm4m-v2/medium_ntp/pt_ns_h512_l8_b8192_mpe1024_tk1e9_gelu_pretrain3.3m_nmlm_mrlinear_mtp0.8_0_0.2_lr3e-4_adp0.1_pdp0_edp0_mdp0_lsi0_short_gated_wd0.1/epoch_51/global_step48830'\n",
      "Detected checkpoint of type zero stage 2, world_size: 1\n",
      "Parsing checkpoint created by deepspeed==0.15.1\n",
      "Reconstructed fp32 state dict with 77 params 37751808 elements\n",
      "[2024-12-16 17:23:28.372732] load ckp using DeepSpeed API `get_fp32_state_dict_from_zero_checkpoint`\n",
      "[2024-12-16 17:23:28.405391] init model params using pytorch `load_state_dict`\n",
      "missing keys: []\n",
      "unexpected_keys: []\n",
      "After loading weights from ckp:\n",
      "GraphGPTConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 20,\n",
      "  \"causal_attention\": true,\n",
      "  \"cls_token_id\": null,\n",
      "  \"dropout\": 0,\n",
      "  \"embed_dim\": 0,\n",
      "  \"embed_pdrop\": 0,\n",
      "  \"eos_token_id\": 19,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_scale_init_value\": 0,\n",
      "  \"loss_type\": null,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"mlp\": [],\n",
      "  \"mlp_pdrop\": 0,\n",
      "  \"model_type\": \"graphgpt\",\n",
      "  \"next_n_token\": 13,\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 8,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_neg\": null,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"path_pdrop\": 0,\n",
      "  \"pooling_method\": \"last\",\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"stack_method\": \"short\",\n",
      "  \"stacked_feat\": 13,\n",
      "  \"stacked_feat_agg_method\": \"gated\",\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 756\n",
      "}\n",
      "\n",
      "model-type: torch.float32\n",
      "\n",
      "GraphGPTCausal(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(756, 512, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (up_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (down_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (act_fn): GELUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=756, bias=False)\n",
      "  (stacked_feat_agg): StackedFeatAggregation(stacked_feat=13, hidden_size=512)\n",
      "  (next_n_token_head): Linear(in_features=512, out_features=6656, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2.21 load from ckp IF provided existing ckp and NOT resume from the ckp\n",
    "ckp, _ = misc_utils.get_latest_ckp(pretrain_cpt)\n",
    "print(f\"Loading pretrained weights from ckp {ckp}\")\n",
    "try:\n",
    "    # fn_model = os.path.join(ckp, \"../model_ema_best.pt\")\n",
    "    # if not os.path.isfile(fn_model):\n",
    "    fn_model = os.path.join(ckp, \"model.pt\")\n",
    "    stat_dict = torch.load(fn_model)\n",
    "    stat_dict = {\n",
    "        (k[7:] if k.startswith(\"module.\") else k): v for k, v in stat_dict.items()\n",
    "    }\n",
    "    print(f\"[{datetime.now()}] load ckp using torch API from:\\n{fn_model}\")\n",
    "except Exception as inst:\n",
    "    # print(type(inst))\n",
    "    # print(inst.args)\n",
    "    print(\"inar: \", inst)\n",
    "    from deepspeed.utils.zero_to_fp32 import (\n",
    "        get_fp32_state_dict_from_zero_checkpoint,\n",
    "    )\n",
    "    stat_dict = get_fp32_state_dict_from_zero_checkpoint(ckp)\n",
    "    print(\n",
    "        f\"[{datetime.now()}] load ckp using DeepSpeed API `get_fp32_state_dict_from_zero_checkpoint`\"\n",
    "    )\n",
    "\n",
    "for key in list(stat_dict.keys()):\n",
    "    if (\"score\" in key) and skip_keys:\n",
    "        stat_dict.pop(key)\n",
    "        print(f\"pop key {key} in stat_dict!\")\n",
    "missing_keys, unexpected_keys = model.load_state_dict(stat_dict, strict=True)\n",
    "print(\n",
    "    f\"[{datetime.now()}] init model params using pytorch `load_state_dict`\\n\"\n",
    "    f\"missing keys: {missing_keys}\\n\"\n",
    "    f\"unexpected_keys: {unexpected_keys}\\n\"\n",
    "    f\"After loading weights from ckp:\\n{model.config}\\nmodel-type: {model.dtype}\\n\\n{model}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-16 17:23:28.418223] Finish -> 3.1 init collator\n"
     ]
    }
   ],
   "source": [
    "pad_to_multiple_of = 8\n",
    "# 3.1 init collator\n",
    "collator_fn = collator.DataCollatorForGSTCausal(\n",
    "    tokenizer=gtokenizer,\n",
    "    max_length=max_position_embeddings,\n",
    "    pad_to_multiple_of=pad_to_multiple_of,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(f\"[{datetime.now()}] Finish -> 3.1 init collator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset_samples_per_epoch: False\n",
      "[2024-12-16 17:23:29.981578] NOT RESET samples of GraphsMapDataset of 3452151 graphs for epoch 0!\n",
      "[2024-12-16 17:23:30.082107] NOT RESET samples of GraphsMapDataset of 3452151 graphs for epoch 1!\n",
      "[2024-12-16 17:23:30.198648] NOT RESET samples of GraphsMapDataset of 3452151 graphs for epoch 2!\n",
      "[2024-12-16 17:23:30.299039] NOT RESET samples of GraphsMapDataset of 3452151 graphs for epoch 3!\n",
      "[2024-12-16 17:23:30.392570] NOT RESET samples of GraphsMapDataset of 3452151 graphs for epoch 4!\n",
      "[2024-12-16 17:23:30.487291] NOT RESET samples of GraphsMapDataset of 3452151 graphs for epoch 5!\n",
      "[2024-12-16 17:23:30.581383] NOT RESET samples of GraphsMapDataset of 3452151 graphs for epoch 6!\n",
      "[2024-12-16 17:23:30.674736] NOT RESET samples of GraphsMapDataset of 3452151 graphs for epoch 7!\n",
      "[2024-12-16 17:23:30.767873] NOT RESET samples of GraphsMapDataset of 3452151 graphs for epoch 8!\n",
      "[2024-12-16 17:23:30.861530] NOT RESET samples of GraphsMapDataset of 3452151 graphs for epoch 9!\n",
      "[2024-12-16 17:23:30.955258] NOT RESET samples of GraphsMapDataset of 3452151 graphs for epoch 10!\n",
      "[2024-12-16 17:23:31.047985] NOT RESET samples of GraphsMapDataset of 3452151 graphs for epoch 11!\n",
      "[2024-12-16 17:23:31.141885] NOT RESET samples of GraphsMapDataset of 3452151 graphs for epoch 12!\n",
      "[2024-12-16 17:23:31.238789] NOT RESET samples of GraphsMapDataset of 3452151 graphs for epoch 13!\n",
      "[2024-12-16 17:23:31.331499] NOT RESET samples of GraphsMapDataset of 3452151 graphs for epoch 14!\n",
      "train_sampler for 15 epochs increase: 3452151 -> 51782265\n"
     ]
    }
   ],
   "source": [
    "# 3.2 set-up loader\n",
    "print(f\"reset_samples_per_epoch: {reset_samples_per_epoch}\")\n",
    "if (not reset_samples_per_epoch) and (\n",
    "    not isinstance(train_dataset, IterableDataset)\n",
    "):\n",
    "    train_sampler_new = []\n",
    "    for epoch in range(epochs):\n",
    "        train_dataset.reset_samples(epoch, rank)\n",
    "        # random.shuffle(train_sampler)\n",
    "        train_sampler_new.extend(train_dataset.sampler)\n",
    "    random.shuffle(train_sampler_new)\n",
    "    print(\n",
    "        f\"train_sampler for {epochs} epochs increase: {len(train_sampler)} -> {len(train_sampler_new)}\"    # train_sampler for 61 epochs increase: 3323391 -> 202726851 3323391* 61\n",
    "    )\n",
    "    train_sampler = train_sampler_new\n",
    "    epochs = 1   # reset to 1 epoch\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=train_shuffle,\n",
    "    sampler=train_sampler,\n",
    "    num_workers=1, # 12\n",
    "    collate_fn=collator_fn,\n",
    "    worker_init_fn=worker_init_fn_seed,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    prefetch_factor=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7fbf69b90ac0>\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "<src.data.dataset_map.GraphsMapDataset object at 0x7fc291e26520>\n",
      "1024\n",
      "1\n",
      "True\n",
      "Total samples: 3452151\n",
      "Batch size: 1024\n",
      "Total batches: 50568\n",
      "[DataLoader Worker 0] seed `random` & `np.random` with 1721667379594961650 & 639804146!\n",
      "Batch: dict_keys(['input_ids', 'position_ids', 'labels', 'attention_mask', 'embed', 'idx'])\n"
     ]
    }
   ],
   "source": [
    "print(train_loader)\n",
    "print(type(train_loader))\n",
    "print(train_loader.dataset)  # Prints the dataset object associated with the DataLoader\n",
    "print(train_loader.batch_size)        # Batch size used\n",
    "print(train_loader.num_workers)       # Number of workers for data loading\n",
    "print(train_loader.drop_last)         # Whether the last incomplete batch is dropped\n",
    "\n",
    "print(f\"Total samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Batch size: {train_loader.batch_size}\")\n",
    "print(f\"Total batches: {len(train_loader)}\")\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(\"Batch:\", batch.keys())\n",
    "\n",
    "# # If the dataset returns (input, label), unpack the batch\n",
    "# inputs, labels = batch\n",
    "# print(\"Inputs:\", inputs)\n",
    "# print(\"Labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 40, 0])\n"
     ]
    }
   ],
   "source": [
    "print(batch['embed'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"device: {device}\")\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DataLoader Worker 0] seed `random` & `np.random` with 4970904486264240564 & 338491828!\n",
      "dict_keys(['input_ids', 'position_ids', 'labels', 'attention_mask', 'embed', 'idx'])\n",
      "tensor([[131, 630, 685, 695, 709, 715, 724, 731, 737, 739, 740, 741, 742],\n",
      "        [132, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "        [133, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "        [134, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "        [135, 630, 685, 695, 709, 714, 724, 731, 737, 739, 746, 748, 755],\n",
      "        [136, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "        [137, 630, 687, 696, 709, 715, 724, 732, 736, 738, 743, 748, 754],\n",
      "        [138, 641, 685, 695, 709, 714, 724, 732, 736, 738, 743, 748, 754],\n",
      "        [139, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754],\n",
      "        [138, 641, 685, 695, 709, 714, 724, 732, 736, 738, 743, 748, 754],\n",
      "        [140, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754],\n",
      "        [138, 641, 685, 695, 709, 714, 724, 732, 736, 738, 743, 748, 754],\n",
      "        [137, 630, 687, 696, 709, 715, 724, 732, 736, 738, 743, 748, 754],\n",
      "        [141, 591, 685, 695, 709, 715, 724, 732, 736, 738, 743, 748, 754],\n",
      "        [142, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "        [143, 630, 685, 696, 709, 717, 724, 732, 736, 738, 743, 748, 754],\n",
      "        [142, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "        [141, 591, 685, 695, 709, 715, 724, 732, 736, 738, 743, 748, 754],\n",
      "        [137, 630, 687, 696, 709, 715, 724, 732, 736, 738, 743, 748, 754],\n",
      "        [136, 630, 685, 696, 709, 716, 724, 732, 736, 738, 743, 748, 754],\n",
      "        [135, 630, 685, 695, 709, 714, 724, 731, 737, 739, 743, 748, 754],\n",
      "        [144, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "        [131, 630, 685, 695, 709, 715, 724, 731, 737, 739, 746, 748, 755],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])\n",
      "tensor([[ 132,  630,  685,  695,  709,  715,  724,  731,  737,  739,  746,  748,\n",
      "          755],\n",
      "        [ 133,  630,  685,  695,  709,  715,  724,  731,  737,  739,  746,  748,\n",
      "          755],\n",
      "        [ 134,  630,  685,  695,  709,  715,  724,  731,  737,  739,  746,  748,\n",
      "          755],\n",
      "        [ 135,  630,  685,  695,  709,  714,  724,  731,  737,  739,  746,  748,\n",
      "          755],\n",
      "        [ 136,  630,  685,  696,  709,  716,  724,  732,  736,  738,  743,  748,\n",
      "          754],\n",
      "        [ 137,  630,  687,  696,  709,  715,  724,  732,  736,  738,  743,  748,\n",
      "          754],\n",
      "        [ 138,  641,  685,  695,  709,  714,  724,  732,  736,  738,  743,  748,\n",
      "          754],\n",
      "        [ 139,  630,  685,  696,  709,  717,  724,  732,  736,  738,  743,  748,\n",
      "          754],\n",
      "        [ 138,  641,  685,  695,  709,  714,  724,  732,  736,  738,  743,  748,\n",
      "          754],\n",
      "        [ 140,  630,  685,  696,  709,  717,  724,  732,  736,  738,  743,  748,\n",
      "          754],\n",
      "        [ 138,  641,  685,  695,  709,  714,  724,  732,  736,  738,  743,  748,\n",
      "          754],\n",
      "        [ 137,  630,  687,  696,  709,  715,  724,  732,  736,  738,  743,  748,\n",
      "          754],\n",
      "        [ 141,  591,  685,  695,  709,  715,  724,  732,  736,  738,  743,  748,\n",
      "          754],\n",
      "        [ 142,  630,  685,  696,  709,  716,  724,  732,  736,  738,  743,  748,\n",
      "          754],\n",
      "        [ 143,  630,  685,  696,  709,  717,  724,  732,  736,  738,  743,  748,\n",
      "          754],\n",
      "        [ 142,  630,  685,  696,  709,  716,  724,  732,  736,  738,  743,  748,\n",
      "          754],\n",
      "        [ 141,  591,  685,  695,  709,  715,  724,  732,  736,  738,  743,  748,\n",
      "          754],\n",
      "        [ 137,  630,  687,  696,  709,  715,  724,  732,  736,  738,  743,  748,\n",
      "          754],\n",
      "        [ 136,  630,  685,  696,  709,  716,  724,  732,  736,  738,  743,  748,\n",
      "          754],\n",
      "        [ 135,  630,  685,  695,  709,  714,  724,  731,  737,  739,  743,  748,\n",
      "          754],\n",
      "        [ 144,  630,  685,  695,  709,  715,  724,  731,  737,  739,  746,  748,\n",
      "          755],\n",
      "        [ 131,  630,  685,  695,  709,  715,  724,  731,  737,  739,  746,  748,\n",
      "          755],\n",
      "        [  19,   19,   19,   19,   19,   19,   19,   19,   19,   19,   19,   19,\n",
      "           19],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100]])\n",
      "torch.Size([1024, 40, 13])\n",
      "torch.Size([1024, 40])\n",
      "torch.Size([1024, 40, 13])\n",
      "torch.Size([1024, 40])\n",
      "torch.Size([1024, 40, 0])\n",
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    print(data.keys())\n",
    "    print(data[\"input_ids\"][0])\n",
    "    print(data[\"labels\"][0])\n",
    "    print(data[\"input_ids\"].shape)\n",
    "    print(data[\"position_ids\"].shape)\n",
    "    print(data[\"labels\"].shape)\n",
    "    print(data[\"attention_mask\"].shape)\n",
    "    print(data[\"embed\"].shape)\n",
    "    print(data[\"idx\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DataLoader Worker 0] seed `random` & `np.random` with 3750542880842102200 & 4285969848!\n",
      "CausalLMOutputWithPast(loss=tensor(0.1341, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.9782, -0.8619, -0.9213,  ..., -0.8311, -0.1390, -0.6392],\n",
      "        [-7.7857, -7.7682, -7.7229,  ..., -7.6117, -2.9909, -3.1718],\n",
      "        [-5.3015, -5.4158, -5.3943,  ..., -5.4932, -1.6270, -1.2912],\n",
      "        ...,\n",
      "        [-0.0352,  0.3066, -0.2495,  ..., -0.4284,  2.0003,  0.2725],\n",
      "        [ 2.2878,  2.6796,  2.9258,  ...,  2.7768,  3.2101,  0.4940],\n",
      "        [-1.0649, -1.2182, -0.7403,  ..., -1.4388,  5.0863,  1.1478]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>), past_key_values=None, hidden_states=None, attentions=None)\n",
      "CausalLMOutputWithPast(loss=tensor(0.1375, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.9172, -0.8996, -0.7361,  ..., -0.9960,  0.4533,  0.2306],\n",
      "        [-8.1157, -8.0207, -8.0581,  ..., -7.9589, -3.0496, -2.8436],\n",
      "        [-4.4264, -4.8304, -4.6915,  ..., -4.8013, -1.0038, -1.0989],\n",
      "        ...,\n",
      "        [-0.8407, -0.5014, -1.0503,  ..., -1.2350,  1.6386, -0.0458],\n",
      "        [ 1.8396,  2.2351,  2.4472,  ...,  2.2881,  3.1137,  0.4318],\n",
      "        [-2.0030, -2.2439, -1.6607,  ..., -2.3330,  6.7403,  2.5745]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>), past_key_values=None, hidden_states=None, attentions=None)\n",
      "CausalLMOutputWithPast(loss=tensor(0.1345, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4921, -0.4297, -0.4237,  ..., -0.6294, -0.4639, -0.2956],\n",
      "        [-7.7886, -7.7363, -7.6792,  ..., -7.5710, -2.7981, -2.8924],\n",
      "        [-5.5810, -5.6474, -5.6452,  ..., -5.7555, -1.6730, -1.3546],\n",
      "        ...,\n",
      "        [-0.5946, -0.2748, -0.8563,  ..., -1.0262,  1.7854,  0.1383],\n",
      "        [ 1.5663,  1.9662,  2.1988,  ...,  2.0669,  2.9826,  0.3655],\n",
      "        [-1.8572, -2.0269, -1.5720,  ..., -2.1453,  4.1471,  3.9380]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>), past_key_values=None, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i, data in enumerate(train_loader):\n",
    "    input_ids = data[\"input_ids\"].to(device)\n",
    "    attention_mask = data[\"attention_mask\"].to(device)\n",
    "    labels = data[\"labels\"].to(device)\n",
    "    inputs_raw_embeds = None\n",
    "    if embed_dim > 0: # in tokenizer config\n",
    "        inputs_raw_embeds = data[\"embed\"].to(device)\n",
    "    output = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=labels,\n",
    "        inputs_raw_embeds=inputs_raw_embeds,\n",
    "    )  # Perform a single forward pass.\n",
    "    print(output)\n",
    "    if i == 2:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
